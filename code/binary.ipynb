{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lovas.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"be23623d337c48f29353c27597e7c582":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_91f82236ec284e4cb0fba9b1052c79db","IPY_MODEL_9a51ab3e63914286b8f8bb89c9aa8b8c","IPY_MODEL_db91d1d6df8e4f90ab55f66c1213d053"],"layout":"IPY_MODEL_6be3a7c180834ab1a2ee8b836dce70f8"}},"91f82236ec284e4cb0fba9b1052c79db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f4ac083797f442481aaf24e582392b6","placeholder":"​","style":"IPY_MODEL_729e6aa355654fd7b69613c10a8595f6","value":"100%"}},"9a51ab3e63914286b8f8bb89c9aa8b8c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d70c1bf64deb4194a48107c37a6325f5","max":178793939,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0f56820be5244d80a70495c17634b064","value":178793939}},"db91d1d6df8e4f90ab55f66c1213d053":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31b983253e7d484cb2c49833c712e746","placeholder":"​","style":"IPY_MODEL_63ff07390db040138cba89a3b74f0cda","value":" 171M/171M [00:01&lt;00:00, 132MB/s]"}},"6be3a7c180834ab1a2ee8b836dce70f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f4ac083797f442481aaf24e582392b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"729e6aa355654fd7b69613c10a8595f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d70c1bf64deb4194a48107c37a6325f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f56820be5244d80a70495c17634b064":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"31b983253e7d484cb2c49833c712e746":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63ff07390db040138cba89a3b74f0cda":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14215,"status":"ok","timestamp":1649946436644,"user":{"displayName":"Xiang Gao","userId":"03678382663657440323"},"user_tz":240},"id":"hV3OfnRxglel","outputId":"25277853-dbb7-460f-f62c-582e18dfe98d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/Shareddrives/EECS 545 Project/5DE/lane-detection-2019-howard')"],"metadata":{"id":"sM6qOXiJK2hf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torch.nn.functional as F\n","class UNetFactory(nn.Module):\n","    \"\"\"\n","    本质上就是一个U型的网络，先encode，后decode，中间可能有架bridge。\n","    其中encoder需要输出skip到decode那边做concatenate，使得decode阶段能补充信息。\n","    bridge不能存在下采样和上采样的操作。\n","    \"\"\"\n","    def __init__(self, encoder_blocks, decoder_blocks, bridge=None):\n","        super(UNetFactory, self).__init__()\n","        self.encoder = UNetEncoder(encoder_blocks)\n","        self.bridge = bridge\n","        self.decoder = UNetDecoder(decoder_blocks)\n","\n","    def forward(self, x):\n","        res = self.encoder(x)\n","        out, skips = res[0], res[1:]\n","        if self.bridge is not None:\n","            out = self.bridge(out)\n","        out = self.decoder(out, skips)\n","        return out\n","\n","class UNetEncoder(nn.Module):\n","    \"\"\"\n","    encoder会有多次下采样，下采样前的feature map要作为skip缓存起来将来送到decoder用。\n","    这里约定，以下采样为界线，将encoder分成多个block，其中第一个block无下采样操作，后面的每个block内都\n","    含有一次下采样操作。\n","    \"\"\"\n","    def __init__(self, blocks):\n","        super(UNetEncoder, self).__init__()\n","        assert len(blocks) > 0\n","        self.blocks = nn.ModuleList(blocks)\n","\n","    def forward(self, x):\n","        skips = []\n","        for i in range(len(self.blocks) - 1):\n","            x = self.blocks[i](x)\n","            skips.append(x)\n","        res = [self.blocks[i+1](x)]\n","        res += skips\n","        return res # 只能以这种方式返回多个tensor\n","\n","class UNetDecoder(nn.Module):\n","    \"\"\"\n","    decoder会有多次上采样，每次上采样后，要跟相应的skip做concatenate。\n","    这里约定，以上采样为界线，将decoder分成多个block，其中最后一个block无上采样操作，其他block内\n","    都含有一次上采样。如此一来，除第一个block以外，其他block都先做concatenate。\n","    \"\"\"\n","    def __init__(self, blocks):\n","        super(UNetDecoder, self).__init__()\n","        assert len(blocks) > 1\n","        self.blocks = nn.ModuleList(blocks)\n","    \n","    def _center_crop(self, skip, x):\n","        \"\"\"\n","        skip和x，谁比较大，就裁剪谁\n","        \"\"\"\n","        _, _, h1, w1 = skip.shape\n","        _, _, h2, w2 = x.shape\n","        ht, wt = min(h1, h2), min(w1, w2)\n","        dh1 = (h1 - ht) // 2 if h1 > ht else 0\n","        dw1 = (w1 - wt) // 2 if w1 > wt else 0\n","        dh2 = (h2 - ht) // 2 if h2 > ht else 0\n","        dw2 = (w2 - wt) // 2 if w2 > wt else 0\n","        return skip[:, :, dh1: (dh1 + ht), dw1: (dw1 + wt)], \\\n","                x[:, :, dh2: (dh2 + ht), dw2: (dw2 + wt)]\n","\n","    def forward(self, x, skips, reverse_skips=True):\n","        assert len(skips) == len(self.blocks) - 1\n","        if reverse_skips:\n","            skips = skips[::-1]\n","        x = self.blocks[0](x)\n","        for i in range(1, len(self.blocks)):\n","            skip, x = self._center_crop(skips[i-1], x)\n","            x = torch.cat([skip, x], dim=1)\n","            x = self.blocks[i](x)\n","        return x\n","\n","def unet_convs(in_channels, out_channels, padding=0):\n","    \"\"\"\n","    unet论文里出现次数最多的2个conv3x3(non-padding)的结构\n","    \"\"\"\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=padding, bias=False),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=padding, bias=False),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True),\n","    )\n","\n","\n","def unet(in_channels, out_channels):\n","    \"\"\"\n","    构造跟论文一致的unet网络\n","    https://arxiv.org/abs/1505.04597\n","    \"\"\"\n","    # encoder\n","    encoder_blocks = [\n","        # two conv3x3\n","        unet_convs(in_channels, 64),\n","        # max pool 2x2, two conv3x3\n","        nn.Sequential(\n","            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n","            unet_convs(64, 128)\n","        ),\n","        # max pool 2x2, two conv3x3\n","        nn.Sequential(\n","            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n","            unet_convs(128, 256)\n","        ),\n","        # max pool 2x2, two conv3x3\n","        nn.Sequential(\n","            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n","            unet_convs(256, 512)\n","        ),\n","        # max pool 2x2\n","        nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n","    ]\n","    # bridge\n","    bridge = nn.Sequential(\n","        # two conv3x3\n","        unet_convs(512, 1024)\n","    )\n","    # decoder\n","    decoder_blocks = [\n","        # up-conv2x2\n","        nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2),\n","        # two conv3x3, up-conv2x2\n","        nn.Sequential(\n","            unet_convs(1024, 512),\n","            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n","        ),\n","        # two conv3x3, up-conv2x2\n","        nn.Sequential(\n","            unet_convs(512, 256),\n","            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n","        ),\n","        # two conv3x3, up-conv2x2\n","        nn.Sequential(\n","            unet_convs(256, 128),\n","            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n","        ),\n","        # two conv3x3, conv1x1\n","        nn.Sequential(\n","            unet_convs(128, 64),\n","            nn.Conv2d(64, out_channels, kernel_size=1)\n","        )\n","    ]\n","    return UNetFactory(encoder_blocks, decoder_blocks, bridge)\n","\n","def unet_resnet(resnet_type, in_channels, out_channels, pretrained=True):\n","    \"\"\"\n","    利用resnet作为encoder，相应地，decoder也做一些改动，使得输出的尺寸跟原始的一致\n","    \"\"\"\n","    if resnet_type == 'resnet18':\n","        resnet = torchvision.models.resnet.resnet18(pretrained)\n","        encoder_out_channels = [in_channels, 64, 64, 128, 256, 512]  # encoder各个block的输出channel\n","    elif resnet_type == 'resnet34':\n","        resnet = torchvision.models.resnet.resnet34(pretrained)\n","        encoder_out_channels = [in_channels, 64, 64, 128, 256, 512]\n","    elif resnet_type == 'resnet50':\n","        resnet = torchvision.models.resnet.resnet50(pretrained)\n","        encoder_out_channels = [in_channels, 64, 256, 512, 1024, 2048]\n","    elif resnet_type == 'resnet101':\n","        resnet = torchvision.models.resnet.resnet101(pretrained)\n","        encoder_out_channels = [in_channels, 64, 256, 512, 1024, 2048]\n","    elif resnet_type == 'resnet152':\n","        resnet = torchvision.models.resnet.resnet152(pretrained)\n","        encoder_out_channels = [in_channels, 64, 256, 512, 1024, 2048]\n","    elif resnet_type == 'resnext50_32x4d':\n","        resnet = torchvision.models.resnet.resnext50_32x4d(pretrained)\n","        encoder_out_channels = [in_channels, 64, 256, 512, 1024, 2048]\n","    else:\n","        raise ValueError(\"unexpected resnet_type\")\n","\n","    # encoder\n","    encoder_blocks = [\n","        # org input\n","        nn.Sequential(),\n","        # conv1\n","        nn.Sequential(\n","            resnet.conv1,\n","            resnet.bn1,\n","            resnet.relu\n","        ),\n","        # conv2_x\n","        nn.Sequential(\n","            resnet.maxpool,\n","            resnet.layer1\n","        ),\n","        # conv3_x\n","        resnet.layer2,\n","        # conv4_x\n","        resnet.layer3,\n","        # conv5_x\n","        resnet.layer4\n","    ]\n","    # bridge\n","    bridge = None  # 感觉并无必要\n","    # decoder\n","    decoder_blocks = []\n","    in_ch = encoder_out_channels[-1]\n","    out_ch = in_ch // 2\n","    decoder_blocks.append(nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)) # up-conv2x2\n","    for i in range(1, len(encoder_blocks)-1):\n","        in_ch = encoder_out_channels[-i-1] + out_ch  # cat\n","        decoder_blocks.append(nn.Sequential(  # two conv3x3, up-conv2x2\n","            unet_convs(in_ch, out_ch, padding=1),\n","            nn.ConvTranspose2d(out_ch, out_ch//2, kernel_size=2, stride=2),\n","        ))\n","        out_ch = out_ch // 2\n","    in_ch = encoder_out_channels[0] + out_ch  # cat\n","    decoder_blocks.append(nn.Sequential(  # two conv3x3, conv1x1\n","        unet_convs(in_ch, out_ch, padding=1),\n","        nn.Conv2d(out_ch, out_channels, kernel_size=1)\n","    ))\n","\n","    return UNetFactory(encoder_blocks, decoder_blocks, bridge)"],"metadata":{"id":"MDTNSK_bQCqL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"rUbpIzPCRmNE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","@description: Configure Class \n","\"\"\"\n","\n","from os.path import join as pjoin\n","from os.path import dirname, abspath\n","import torch\n","\n","class ConfigTrain(object):\n","    # 目录\n","    PROJECT_ROOT = \"/content/drive/Shareddrives/EECS 545 Project/5DE/lane-detection-2019-howard\"\n","    DATA_LIST_ROOT = pjoin(PROJECT_ROOT, 'data_list')\n","    TRAIN_ROOT = \"/content/drive/Shareddrives/EECS 545 Project/data\"\n","    IMAGE_ROOT = pjoin(TRAIN_ROOT, 'Image_Data')\n","    LABEL_ROOT = pjoin(TRAIN_ROOT, 'Gray_Label')\n","    WEIGHTS_ROOT = pjoin(PROJECT_ROOT, 'weights')\n","    WEIGHTS_SAVE_ROOT = pjoin(WEIGHTS_ROOT, 'result')\n","    LOG_ROOT = pjoin(PROJECT_ROOT, 'logs')\n","\n","    # log文件\n","    # LOG_SUSPICIOUS_FILES = pjoin(LOG_ROOT, 'suspicious_files.log')\n","    # LOG_SUSPICIOUS_FILES = pjoin(LOG_ROOT, 'suspicious_files_b1.log')\n","    # LOG_SUSPICIOUS_FILES = pjoin(LOG_ROOT, 'suspicious_files_b4.log')\n","    LOG_SUSPICIOUS_FILES = pjoin(LOG_ROOT, 'suspicious_files_b2.log')\n","\n","    # 设备\n","    DEVICE = 'cuda:0' \n","    \n","    if torch.cuda.is_available():\n","      print(\"Using the GPU. You are good to go!\")\n","      DEVICE = 'cuda'\n","    else:\n","      print(\"Using the CPU. Overall speed may be slowed down\")\n","      DEVICE = 'cpu'\n","    \n","    # 网络类型\n","    NET_NAME = 'unet_resnet101'\n","    #NET_NAME = 'resnext50_32x4d'\n","    # 网络参数\n","    NUM_CLASSES = 8  # 8个类别\n","    IMAGE_SIZE = (768, 256)  # 训练的图片的尺寸(h,w)\n","    # IMAGE_SIZE = (1024, 384)  # 训练的图片的尺寸(h,w)\n","    #IMAGE_SIZE = (1536, 512)  # 训练的图片的尺寸(h,w)\n","    HEIGHT_CROP_OFFSET = 690  # 在height方向上将原图裁掉的offset\n","    # BATCH_SIZE = 8  # 数据批次大小\n","    # BATCH_SIZE = 1  # 数据批次大小\n","    # BATCH_SIZE = 4  # 数据批次大小\n","    BATCH_SIZE = 4  # 数据批次大小\n","    EPOCH_NUM = 4  # 总轮次\n","    PRETRAIN = False # 是否加载预训练的权重\n","    EPOCH_BEGIN = 0  # 接着前面的epoch训练，默认0，表示从头训练\n","    PRETRAINED_WEIGHTS = pjoin(WEIGHTS_ROOT, '1024x384_b4_unet_resnext50_32x4d', 'result_6.pt')\n","    BASE_LR = 0.001  # 学习率\n","    LR_STRATEGY = [\n","        [0.001], # epoch 0\n","        [0.001], # epoch 1\n","        [0.001], # epoch 2\n","        [0.001, 0.0006, 0.0003, 0.0001, 0.0004, 0.0008, 0.001], # epoch 3\n","        [0.001, 0.0006, 0.0003, 0.0001, 0.0004, 0.0008, 0.001], # epoch 4\n","        [0.001, 0.0006, 0.0003, 0.0001, 0.0004, 0.0008, 0.001], # epoch 5\n","        [0.0004, 0.0003, 0.0002, 0.0001, 0.0002, 0.0003, 0.0004], # epoch 6\n","        [0.0004, 0.0003, 0.0002, 0.0001, 0.0002, 0.0003, 0.0004], # epoch 7\n","    ]\n","    SUSPICIOUS_RATE = 0.8  # 可疑比例：当某个iteration的miou比当前epoch_miou的可疑比例还要小的时候，记录此次iteration的训练数据索引，人工排查是否数据有问题\n","\n","## TO DO: Define PROJECT_ROOT##\n","'''    \n","class ConfigInference(object):\n","    # 目录\n","    PROJECT_ROOT = dirname(abspath(__file__)) \n","    DATA_ROOT = pjoin(PROJECT_ROOT, 'data')\n","    IMAGE_ROOT = pjoin(DATA_ROOT, 'TestImage')\n","    LABEL_ROOT = pjoin(DATA_ROOT, 'TestLabel')\n","    OVERLAY_ROOT = pjoin(DATA_ROOT, 'TestOverlay')\n","    WEIGHTS_ROOT = pjoin(PROJECT_ROOT, 'weights')\n","    PRETRAINED_WEIGHTS = pjoin(WEIGHTS_ROOT, '1024x384_b4_unet_resnext50_32x4d', 'resnext50_32x4d-7cdf4587.pth')\n","    LOG_ROOT = pjoin(PROJECT_ROOT, 'logs')\n","\n","    # 设备\n","    DEVICE = 'cuda:0'\n","\n","    # 网络类型\n","    NET_NAME = 'resnext50_32x4d'\n","\n","    # 网络参数\n","    NUM_CLASSES = 8  # 8个类别\n","    # IMAGE_SIZE = (768, 256)  # 训练的图片的尺寸(h,w)\n","    IMAGE_SIZE = (1024, 384)  # 训练的图片的尺寸(h,w)\n","    # IMAGE_SIZE = (1536, 512)  # 训练的图片的尺寸(h,w)\n","    HEIGHT_CROP_OFFSET = 690  # 在height方向上将原图裁掉的offset\n","    BATCH_SIZE = 1  # 数据批次大小\n","\n","    # 原图的大小\n","    IMAGE_SIZE_ORG = (3384, 1710)\n","'''    "],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"dha6U_LzGxtJ","executionInfo":{"status":"ok","timestamp":1649946446562,"user_tz":240,"elapsed":275,"user":{"displayName":"Xiang Gao","userId":"03678382663657440323"}},"outputId":"dc13da33-3f3f-4e5f-d697-bbd4421e73f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using the GPU. You are good to go!\n"]},{"output_type":"execute_result","data":{"text/plain":["\"    \\nclass ConfigInference(object):\\n    # 目录\\n    PROJECT_ROOT = dirname(abspath(__file__)) \\n    DATA_ROOT = pjoin(PROJECT_ROOT, 'data')\\n    IMAGE_ROOT = pjoin(DATA_ROOT, 'TestImage')\\n    LABEL_ROOT = pjoin(DATA_ROOT, 'TestLabel')\\n    OVERLAY_ROOT = pjoin(DATA_ROOT, 'TestOverlay')\\n    WEIGHTS_ROOT = pjoin(PROJECT_ROOT, 'weights')\\n    PRETRAINED_WEIGHTS = pjoin(WEIGHTS_ROOT, '1024x384_b4_unet_resnext50_32x4d', 'resnext50_32x4d-7cdf4587.pth')\\n    LOG_ROOT = pjoin(PROJECT_ROOT, 'logs')\\n\\n    # 设备\\n    DEVICE = 'cuda:0'\\n\\n    # 网络类型\\n    NET_NAME = 'resnext50_32x4d'\\n\\n    # 网络参数\\n    NUM_CLASSES = 8  # 8个类别\\n    # IMAGE_SIZE = (768, 256)  # 训练的图片的尺寸(h,w)\\n    IMAGE_SIZE = (1024, 384)  # 训练的图片的尺寸(h,w)\\n    # IMAGE_SIZE = (1536, 512)  # 训练的图片的尺寸(h,w)\\n    HEIGHT_CROP_OFFSET = 690  # 在height方向上将原图裁掉的offset\\n    BATCH_SIZE = 1  # 数据批次大小\\n\\n    # 原图的大小\\n    IMAGE_SIZE_ORG = (3384, 1710)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["'''\n","Define loss\n","'''\n","class MySoftmaxCrossEntropyLoss(nn.Module):\n","\n","    def __init__(self, nbclasses):\n","        super(MySoftmaxCrossEntropyLoss, self).__init__()\n","        self.nbclasses = nbclasses\n","\n","    def forward(self, inputs, target):\n","        if inputs.dim() > 2:\n","            inputs = inputs.view(inputs.size(0), inputs.size(1), -1)  # N,C,H,W => N,C,H*W\n","            inputs = inputs.transpose(1, 2)  # N,C,H*W => N,H*W,C\n","            inputs = inputs.contiguous().view(-1, self.nbclasses)  # N,H*W,C => N*H*W,C\n","        target = target.view(-1)\n","        return nn.CrossEntropyLoss(reduction=\"mean\")(inputs, target)\n","\n","\n","class FocalLoss(nn.Module):\n","\n","    def __init__(self, gamma=0, alpha=None, size_average=True):\n","        super(FocalLoss, self).__init__()\n","        self.gamma = gamma\n","        self.alpha = alpha\n","        self.alpha = torch.tensor([alpha, 1 - alpha])\n","        self.size_average = size_average\n","\n","    def forward(self, inputs, target):\n","        if inputs.dim() > 2:\n","            inputs = inputs\n","            inputs = inputs.view(inputs.size(0), inputs.size(1), -1)  # N,C,H,W => N,C,H*W\n","            inputs = inputs.transpose(1, 2)  # N,C,H*W => N,H*W,C\n","            inputs = inputs.contiguous().view(-1, inputs.size(2))  # N,H*W,C => N*H*W,C\n","        target = target.view(-1, 1)\n","\n","        logpt = F.log_softmax(inputs,dim=1)\n","        logpt = logpt.gather(1, target)\n","        logpt = logpt.view(-1)\n","        pt = logpt.exp()\n","\n","        if self.alpha is not None:\n","            if self.alpha.type() != inputs.data.type():\n","                self.alpha = self.alpha.type_as(inputs.data)\n","            at = self.alpha.gather(0, target.view(-1))\n","            logpt = logpt * at\n","        # mask = mask.view(-1)\n","        loss = -1 * (1 - pt) ** self.gamma * logpt #* mask\n","        if self.size_average:\n","            return loss.mean()\n","        else:\n","            return loss.sum()\n","\n","\n","def make_one_hot(input, num_classes):\n","    \"\"\"Convert class index tensor to one hot encoding tensor.\n","    Args:\n","         input: A tensor of shape [N, 1, *]\n","         num_classes: An int of number of class\n","    Returns:\n","        A tensor of shape [N, num_classes, *]\n","    \"\"\"\n","    shape = np.array(input.shape)\n","    shape[1] = num_classes\n","    shape = tuple(shape)\n","    result = torch.zeros(shape)\n","    result = result.scatter_(1, input.cpu(), 1)\n","\n","    return result\n","\n","\n","class BinaryDiceLoss(nn.Module):\n","    \"\"\"Dice loss of binary class\n","    Args:\n","        smooth: A float number to smooth loss, and avoid NaN error, default: 1\n","        p: Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2\n","        predict: A tensor of shape [N, *]\n","        target: A tensor of shape same with predict log文件\n","    # LOG_SUSPICIOUS_FILES = pjoin(LOG_ROOT, 'suspicious_files.log')\n","    # LOG_SUSPICIOUS_FILES = pjoin(LOG_ROOT, 'suspicious_files_b1.log')\n","    # LOG_SUSPICIOUS_FILES = pjoin(LOG_ROOT, 'suspicious_files_b4.log')\n","    LOG_SUSPICIOUS_FILES = pjoin(LOG_ROOT, 'suspicious_files_b2.log')\n","        reduction: Reduction method to apply, return mean over batch if 'mean',\n","            return sum if 'sum', return a tensor of shape [N,] if 'none'\n","    Returns:\n","        Loss tensor according to arg reduction\n","    Raise:\n","        Exception if unexpected reduction\n","    \"\"\"\n","    def __init__(self, smooth=1, p=2, reduction='mean'):\n","        super(BinaryDiceLoss, self).__init__()\n","        self.smooth = smooth\n","        self.p = p\n","        self.reduction = reduction\n","\n","    def forward(self, predict, target):\n","        assert predict.shape[0] == target.shape[0], \"predict & target batch size don't match\"\n","        predict = predict.contiguous().view(predict.shape[0], -1)\n","        target = target.contiguous().view(target.shape[0], -1)\n","        num = 2*torch.sum(torch.mul(predict, target), dim=1) + self.smooth\n","        den = torch.sum(predict.pow(self.p) + target.pow(self.p), dim=1) + self.smooth\n","\n","        loss = 1 - num / den\n","\n","        if self.reduction == 'mean':\n","            return loss.mean()\n","        elif self.reduction == 'sum':\n","            return loss.sum()\n","        elif self.reduction == 'none':\n","            return loss\n","        else:\n","            raise Exception('Unexpected reduction {}'.format(self.reduction))\n","\n","\n","class DiceLoss(nn.Module):\n","    \"\"\"Dice loss, need one hot encode input\n","    Args:\n","        weight: An array of shape [num_classes,]\n","        ignore_index: class index to ignore\n","        predict: A tensor of shape [N, C, *]\n","        target: A tensor of same shape with predict\n","        other args pass to BinaryDiceLoss\n","    Return:\n","        same as BinaryDiceLoss\n","    \"\"\"\n","    def __init__(self, weight=None, ignore_index=None, **kwargs):\n","        super(DiceLoss, self).__init__()\n","        self.kwargs = kwargs\n","        self.weight = weight\n","        self.ignore_index = ignore_index\n","\n","    def forward(self, predict, target):\n","        assert predict.shape == target.shape, 'predict & target shape do not match'\n","        dice = BinaryDiceLoss(**self.kwargs)\n","        total_loss = 0\n","        predict = F.softmax(predict, dim=1)\n","\n","        for i in range(target.shape[1]):\n","            if i != self.ignore_index:\n","                dice_loss = dice(predict[:, i], target[:, i])\n","                if self.weight is not None:\n","                    assert self.weight.shape[0] == target.shape[1], \\\n","                        'Expect weight shape [{}], get[{}]'.format(target.shape[1], self.weight.shape[0])\n","                    dice_loss *= self.weights[i]\n","                total_loss += dice_loss\n","\n","        return total_loss/target.shape[1]\n","\n"],"metadata":{"id":"VhCWX0QNWYYI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\"\"\"\n","Lovasz-Softmax and Jaccard hinge loss in PyTorch\n","Maxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n","\"\"\"\n","\n","from __future__ import print_function, division\n","\n","import torch\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","import numpy as np\n","try:\n","    from itertools import  ifilterfalse\n","except ImportError: # py3k\n","    from itertools import  filterfalse as ifilterfalse\n","\n","\n","def lovasz_grad(gt_sorted):\n","    \"\"\"\n","    Computes gradient of the Lovasz extension w.r.t sorted errors\n","    See Alg. 1 in paper\n","    \"\"\"\n","    p = len(gt_sorted)\n","    gts = gt_sorted.sum()\n","    intersection = gts - gt_sorted.float().cumsum(0)\n","    union = gts + (1 - gt_sorted).float().cumsum(0)\n","    jaccard = 1. - intersection / union\n","    if p > 1: # cover 1-pixel case\n","        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n","    return jaccard\n","\n","\n","def iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n","    \"\"\"\n","    IoU for foreground class\n","    binary: 1 foreground, 0 background\n","    \"\"\"\n","    if not per_image:\n","        preds, labels = (preds,), (labels,)\n","    ious = []\n","    for pred, label in zip(preds, labels):\n","        intersection = ((label == 1) & (pred == 1)).sum()\n","        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n","        if not union:\n","            iou = EMPTY\n","        else:\n","            iou = float(intersection) / float(union)\n","        ious.append(iou)\n","    iou = mean(ious)    # mean accross images if per_image\n","    return 100 * iou\n","\n","\n","def iou(preds, labels, C, EMPTY=1., ignore=None, per_image=False):\n","    \"\"\"\n","    Array of IoU for each (non ignored) class\n","    \"\"\"\n","    if not per_image:\n","        preds, labels = (preds,), (labels,)\n","    ious = []\n","    for pred, label in zip(preds, labels):\n","        iou = []    \n","        for i in range(C):\n","            if i != ignore: # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n","                intersection = ((label == i) & (pred == i)).sum()\n","                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n","                if not union:\n","                    iou.append(EMPTY)\n","                else:\n","                    iou.append(float(intersection) / float(union))\n","        ious.append(iou)\n","    ious = [mean(iou) for iou in zip(*ious)] # mean accross images if per_image\n","    return 100 * np.array(ious)\n","\n","\n","# --------------------------- BINARY LOSSES ---------------------------\n","\n","\n","def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n","    \"\"\"\n","    Binary Lovasz hinge loss\n","      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n","      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n","      per_image: compute the loss per image instead of per batch\n","      ignore: void class id\n","    \"\"\"\n","    if per_image:\n","        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n","                          for log, lab in zip(logits, labels))\n","    else:\n","        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n","    return loss\n","\n","\n","def lovasz_hinge_flat(logits, labels):\n","    \"\"\"\n","    Binary Lovasz hinge loss\n","      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n","      labels: [P] Tensor, binary ground truth labels (0 or 1)\n","      ignore: label to ignore\n","    \"\"\"\n","    if len(labels) == 0:\n","        # only void pixels, the gradients should be 0\n","        return logits.sum() * 0.\n","    signs = 2. * labels.float() - 1.\n","    errors = (1. - logits * Variable(signs))\n","    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n","    perm = perm.data\n","    gt_sorted = labels[perm]\n","    grad = lovasz_grad(gt_sorted)\n","    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n","    return loss\n","\n","\n","def flatten_binary_scores(scores, labels, ignore=None):\n","    \"\"\"\n","    Flattens predictions in the batch (binary case)\n","    Remove labels equal to 'ignore'\n","    \"\"\"\n","    scores = scores.view(-1)\n","    labels = labels.view(-1)\n","    if ignore is None:\n","        return scores, labels\n","    valid = (labels != ignore)\n","    vscores = scores[valid]\n","    vlabels = labels[valid]\n","    return vscores, vlabels\n","\n","\n","class StableBCELoss(torch.nn.modules.Module):\n","    def __init__(self):\n","         super(StableBCELoss, self).__init__()\n","    def forward(self, input, target):\n","         neg_abs = - input.abs()\n","         loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n","         return loss.mean()\n","\n","\n","def binary_xloss(logits, labels, ignore=None):\n","    \"\"\"\n","    Binary Cross entropy loss\n","      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n","      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n","      ignore: void class id\n","    \"\"\"\n","    logits, labels = flatten_binary_scores(logits, labels, ignore)\n","    loss = StableBCELoss()(logits, Variable(labels.float()))\n","    return loss\n","\n","\n","# --------------------------- MULTICLASS LOSSES ---------------------------\n","\n","\n","def lovasz_softmax(probas, labels, classes='present', per_image=False, ignore=None):\n","    \"\"\"\n","    Multi-class Lovasz-Softmax loss\n","      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1).\n","              Interpreted as binary (sigmoid) output with outputs of size [B, H, W].\n","      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n","      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n","      per_image: compute the loss per image instead of per batch\n","      ignore: void class labels\n","    \"\"\"\n","    if per_image:\n","        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), classes=classes)\n","                          for prob, lab in zip(probas, labels))\n","    else:\n","        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), classes=classes)\n","    return loss\n","\n","\n","def lovasz_softmax_flat(probas, labels, classes='present'):\n","    \"\"\"\n","    Multi-class Lovasz-Softmax loss\n","      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n","      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n","      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n","    \"\"\"\n","    if probas.numel() == 0:\n","        # only void pixels, the gradients should be 0\n","        return probas * 0.\n","    C = probas.size(1)\n","    losses = []\n","    class_to_sum = list(range(C)) if classes in ['all', 'present'] else classes\n","    for c in class_to_sum:\n","        fg = (labels == c).float() # foreground for class c\n","        if (classes is 'present' and fg.sum() == 0):\n","            continue\n","        if C == 1:\n","            if len(classes) > 1:\n","                raise ValueError('Sigmoid output possible only with 1 class')\n","            class_pred = probas[:, 0]\n","        else:\n","            class_pred = probas[:, c]\n","        errors = (Variable(fg) - class_pred).abs()\n","        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n","        perm = perm.data\n","        fg_sorted = fg[perm]\n","        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n","    return mean(losses)\n","\n","\n","def flatten_probas(probas, labels, ignore=None):\n","    \"\"\"\n","    Flattens predictions in the batch\n","    \"\"\"\n","    if probas.dim() == 3:\n","        # assumes output of a sigmoid layer\n","        B, H, W = probas.size()\n","        probas = probas.view(B, 1, H, W)\n","    B, C, H, W = probas.size()\n","    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n","    labels = labels.view(-1)\n","    if ignore is None:\n","        return probas, labels\n","    valid = (labels != ignore)\n","    vprobas = probas[valid.nonzero().squeeze()]\n","    vlabels = labels[valid]\n","    return vprobas, vlabels\n","\n","def xloss(logits, labels, ignore=None):\n","    \"\"\"\n","    Cross entropy loss\n","    \"\"\"\n","    return F.cross_entropy(logits, Variable(labels), ignore_index=255)\n","\n","\n","# --------------------------- HELPER FUNCTIONS ---------------------------\n","def isnan(x):\n","    return x != x\n","    \n","    \n","def mean(l, ignore_nan=False, empty=0):\n","    \"\"\"\n","    nanmean compatible with generators.\n","    \"\"\"\n","    l = iter(l)\n","    if ignore_nan:\n","        l = ifilterfalse(isnan, l)\n","    try:\n","        n = 1\n","        acc = next(l)\n","    except StopIteration:\n","        if empty == 'raise':\n","            raise ValueError('Empty mean')\n","        return empty\n","    for n, v in enumerate(l, 2):\n","        acc += v\n","    if n == 1:\n","        return acc\n","    return acc / n"],"metadata":{"id":"EAFofjy4ekge"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","util files\n","'''\n","def create_net(in_channels, out_channels, net_name='unet'):\n","    \"\"\"\n","    创建网络\n","    :param in_channels: 输入通道数\n","    :param out_channels: 输出通道数\n","    # :param net_name: 网络类型，可选 unet | unet_resnet18/34/50/101/152 |unet_resnext50_32x4d | deeplabv3p\n","    :param net_name: 网络类型，可选 unet | unet_resnet34\n","    \"\"\"\n","    if net_name == 'unet':\n","        net = unet(in_channels, out_channels)\n","    elif net_name == 'unet_resnet34':\n","        net = unet_resnet('resnet34', in_channels, out_channels)\n","    elif net_name == 'unet_resnet50':\n","        net = unet_resnet('resnet50', in_channels, out_channels)\n","    elif net_name == 'unet_resnet101':\n","        net = unet_resnet('resnet101', in_channels, out_channels)    \n","    elif net_name == 'resnext50_32x4d':\n","        net = unet_resnet('resnext50_32x4d', in_channels, out_channels)\n","    else:\n","        raise ValueError('Not supported net_name: {}'.format(net_name))\n","\n","    return net\n","\n","def create_loss(predicts: torch.Tensor, labels: torch.Tensor, num_classes):\n","    \"\"\"\n","    创建loss\n","    @param predicts: shape=(n, c, h, w)\n","    @param labels: shape=(n, h, w) or shape=(n, 1, h, w)\n","    @param num_classes: int should equal to channels of predicts\n","    @return: loss, mean_iou\n","    \"\"\"\n","    # permute to (n, h, w, c)\n","    predicts = predicts.permute((0, 2, 3, 1))\n","    # reshape to (-1, num_classes)  每个像素在每种分类上都有一个概率\n","    predicts = predicts.reshape((-1, num_classes))\n","    ##print(predicts.shape)\n","    ##print(labels.flatten().shape)\n","    # BCE with DICE\n","    bce_loss = F.cross_entropy(predicts, labels.flatten(), reduction='mean')  # 函数内会自动做softmax\n","    \n","    # 将labels做one_hot处理，得到的形状跟predicts相同\n","    labels_one_hot = utils.make_one_hot(labels.reshape((-1, 1)), num_classes)\n","    dice_loss = utils.DiceLoss()(predicts, labels_one_hot.to(labels.device))  # torch没有原生的，从老师给的代码里拿过来用\n","    #loss = bce_loss + dice_loss\n","    loss = bce_loss\n","    ious = compute_iou(predicts, labels.reshape((-1, 1)), num_classes)\n","    return loss, torch.mean(ious)\n","\n","def compute_iou(predicts, labels, num_classes):\n","    \"\"\"\n","    计算iou\n","    @param predicts: shape=(-1, classes)\n","    @param labels: shape=(-1, 1)\n","    \"\"\"\n","    ious = torch.zeros(num_classes)\n","    predicts = F.softmax(predicts, dim=1)\n","    predicts = torch.argmax(predicts, dim=1, keepdim=True)\n","    for i in range(num_classes):\n","        intersect = torch.sum((predicts == i) * (labels == i))\n","        area = torch.sum(predicts == i) + torch.sum(labels == i) - intersect\n","        ious[i] = intersect / (area + 1e-6)\n","    return ious\n"],"metadata":{"id":"b7tOt0NiQTyj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"y2sty2Y2Zx9s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","@description: 执行训练\n","\"\"\"\n","\n","\n","\"\"\"\n","import\n","\"\"\"\n","#from config import ConfigTrain\n","import utils\n","from os.path import join as pjoin\n","import pandas as pd\n","import numpy as np\n","import cv2\n","import torch\n","import time\n","\n","\"\"\"\n","main\n","\"\"\"\n","from tqdm import tqdm\n","if __name__ == '__main__':\n","    cfg = ConfigTrain()\n","    print('Pick device: ', cfg.DEVICE)\n","    device = torch.device(cfg.DEVICE)\n","\n","    # 网络\n","    print('Generating net: ', cfg.NET_NAME)\n","    net = create_net(3, cfg.NUM_CLASSES, net_name=cfg.NET_NAME)\n","    if cfg.PRETRAIN:  # 加载预训练权重\n","        print('Load pretrain weights: ', cfg.PRETRAINED_WEIGHTS)\n","        net.load_state_dict(torch.load(cfg.PRETRAINED_WEIGHTS, map_location='cpu'))\n","    net.to(device)\n","    # 优化器\n","    optimizer = torch.optim.Adam(net.parameters(), lr=cfg.BASE_LR) \n","\n","    # 训练数据生成器\n","    print('Preparing data... batch_size: {}, image_size: {}, crop_offset: {}'.format(cfg.BATCH_SIZE, cfg.IMAGE_SIZE, cfg.HEIGHT_CROP_OFFSET))\n","    df_train = pd.read_csv(pjoin(cfg.DATA_LIST_ROOT, 'train.csv'))\n","    data_generator = utils.train_data_generator(np.array(df_train['image']),\n","                                                np.array(df_train['label']),\n","                                                cfg.BATCH_SIZE, cfg.IMAGE_SIZE, cfg.HEIGHT_CROP_OFFSET)\n","\n","    # 训练\n","    print('Let us train ...')\n","    log_iters = 1  # 多少次迭代打印一次log\n","    epoch_size = int(len(df_train) / cfg.BATCH_SIZE)  # 一个轮次包含的迭代次数\n","    ##trn_loss_hist = []\n","    ##iou_hist = []\n","    loss_plot = []\n","    iou_plot = []\n","    for epoch in range(cfg.EPOCH_BEGIN, cfg.EPOCH_NUM):\n","        epoch_loss = 0.0\n","        epoch_miou = 0.0\n","        last_epoch_miou = 0.0\n","        prev_time = time.time()\n","        for iteration in tqdm(range(1 , epoch_size + 1)):\n","            images, labels, images_filename = next(data_generator)\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            lr = utils.ajust_learning_rate(optimizer, cfg.LR_STRATEGY, epoch, iteration-1, epoch_size)\n","\n","            predicts = net(images)\n","\n","            optimizer.zero_grad()\n","\n","            # create loss\n","            cross_loss, mean_iou = utils.create_loss(predicts, labels, cfg.NUM_CLASSES)\n","            #iou = utils.iou(predicts, labels, 3,ignore=255, per_image=True)\n","            predicts =  torch.nn.functional.softmax(predicts,dim=1)\n","            #f_loss = focal_loss(predicts,labels)\n","            \n","            loss_lovasz_softmax = utils.lovasz_softmax(predicts, labels)\n","            loss = cross_loss + loss_lovasz_softmax\n","\n","            epoch_loss += loss.item()\n","            epoch_miou += mean_iou.item()\n","\n","            print(\"[Epoch-%d Iter-%d] LR: %.4f: iter loss: %.3f, iter iou: %.3f, epoch loss: %.3f, epoch iou: %.3f,  time cost: %.3f s\"\n","                % (epoch, iteration, lr, loss.item(), mean_iou.item(), epoch_loss / iteration, epoch_miou / iteration, time.time() - prev_time))\n","            prev_time = time.time()\n","\n","            # if mean_iou.item() < last_epoch_miou * cfg.SUSPICIOUS_RATE:\n","            #   ## TO DO: define log file or create a log file##\n","            #     with open(cfg.LOG_SUSPICIOUS_FILES, 'a+') as f:\n","            #         for filename in images_filename:\n","            #             f.write(\"{}\\n\".format(filename))\n","            #         f.flush()\n","\n","            # last_epoch_miou = epoch_miou / iteration\n","            \n","            loss.backward()\n","            loss_plot.append(loss.item())\n","            iou_plot.append(mean_iou.item())\n","            optimizer.step()\n","\n","        torch.save(net.state_dict(), \n","                    pjoin(cfg.WEIGHTS_SAVE_ROOT, \"weights_ep_%d_%.3f_%.3f.pth\" \n","                            % (epoch, epoch_loss / epoch_size, epoch_miou / epoch_size)))\n","    \n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["be23623d337c48f29353c27597e7c582","91f82236ec284e4cb0fba9b1052c79db","9a51ab3e63914286b8f8bb89c9aa8b8c","db91d1d6df8e4f90ab55f66c1213d053","6be3a7c180834ab1a2ee8b836dce70f8","9f4ac083797f442481aaf24e582392b6","729e6aa355654fd7b69613c10a8595f6","d70c1bf64deb4194a48107c37a6325f5","0f56820be5244d80a70495c17634b064","31b983253e7d484cb2c49833c712e746","63ff07390db040138cba89a3b74f0cda"]},"id":"gl-yCIdGOCQN","outputId":"20e5e572-0fed-4368-d06a-7865470f7d55"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pick device:  cuda\n","Generating net:  unet_resnet101\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/171M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be23623d337c48f29353c27597e7c582"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Preparing data... batch_size: 4, image_size: (768, 256), crop_offset: 690\n","Let us train ...\n","torch.Size([786432, 8])\n","torch.Size([786432])\n","[Epoch-0 Iter-1] LR: 0.0010: iter loss: 3.291, iter iou: 0.008, epoch loss: 3.291, epoch iou: 0.008,  time cost: 39.705 s\n","torch.Size([786432, 8])\n","torch.Size([786432])\n","[Epoch-0 Iter-2] LR: 0.0010: iter loss: 3.136, iter iou: 0.011, epoch loss: 3.213, epoch iou: 0.009,  time cost: 22.837 s\n","torch.Size([786432, 8])\n","torch.Size([786432])\n","[Epoch-0 Iter-3] LR: 0.0010: iter loss: 2.918, iter iou: 0.062, epoch loss: 3.115, epoch iou: 0.027,  time cost: 26.775 s\n","torch.Size([786432, 8])\n","torch.Size([786432])\n","[Epoch-0 Iter-4] LR: 0.0010: iter loss: 2.778, iter iou: 0.083, epoch loss: 3.030, epoch iou: 0.041,  time cost: 17.230 s\n","torch.Size([786432, 8])\n","torch.Size([786432])\n","[Epoch-0 Iter-5] LR: 0.0010: iter loss: 2.632, iter iou: 0.102, epoch loss: 2.951, epoch iou: 0.053,  time cost: 21.153 s\n","torch.Size([786432, 8])\n","torch.Size([786432])\n","[Epoch-0 Iter-6] LR: 0.0010: iter loss: 2.563, iter iou: 0.104, epoch loss: 2.886, epoch iou: 0.062,  time cost: 13.728 s\n","torch.Size([786432, 8])\n","torch.Size([786432])\n","[Epoch-0 Iter-7] LR: 0.0010: iter loss: 2.448, iter iou: 0.115, epoch loss: 2.824, epoch iou: 0.069,  time cost: 14.658 s\n","torch.Size([786432, 8])\n","torch.Size([786432])\n","[Epoch-0 Iter-8] LR: 0.0010: iter loss: 2.392, iter iou: 0.113, epoch loss: 2.770, epoch iou: 0.075,  time cost: 12.031 s\n","torch.Size([786432, 8])\n","torch.Size([786432])\n","[Epoch-0 Iter-9] LR: 0.0010: iter loss: 2.320, iter iou: 0.119, epoch loss: 2.720, epoch iou: 0.080,  time cost: 12.751 s\n","torch.Size([786432, 8])\n","torch.Size([786432])\n","[Epoch-0 Iter-10] LR: 0.0010: iter loss: 2.280, iter iou: 0.118, epoch loss: 2.676, epoch iou: 0.084,  time cost: 12.391 s\n","torch.Size([786432, 8])\n","torch.Size([786432])\n","[Epoch-0 Iter-11] LR: 0.0010: iter loss: 2.223, iter iou: 0.125, epoch loss: 2.635, epoch iou: 0.087,  time cost: 13.206 s\n","torch.Size([786432, 8])\n","torch.Size([786432])\n","[Epoch-0 Iter-12] LR: 0.0010: iter loss: 2.211, iter iou: 0.119, epoch loss: 2.599, epoch iou: 0.090,  time cost: 15.339 s\n","torch.Size([786432, 8])\n","torch.Size([786432])\n","[Epoch-0 Iter-13] LR: 0.0010: iter loss: 2.146, iter iou: 0.132, epoch loss: 2.564, epoch iou: 0.093,  time cost: 12.772 s\n","torch.Size([786432, 8])\n","torch.Size([786432])\n","[Epoch-0 Iter-14] LR: 0.0010: iter loss: 2.131, iter iou: 0.118, epoch loss: 2.533, epoch iou: 0.095,  time cost: 11.802 s\n","torch.Size([786432, 8])\n","torch.Size([786432])\n","[Epoch-0 Iter-15] LR: 0.0010: iter loss: 2.064, iter iou: 0.140, epoch loss: 2.502, epoch iou: 0.098,  time cost: 13.521 s\n","torch.Size([786432, 8])\n","torch.Size([786432])\n","[Epoch-0 Iter-16] LR: 0.0010: iter loss: 2.057, iter iou: 0.126, epoch loss: 2.474, epoch iou: 0.100,  time cost: 12.123 s\n","torch.Size([786432, 8])\n","torch.Size([786432])\n","[Epoch-0 Iter-17] LR: 0.0010: iter loss: 2.042, iter iou: 0.118, epoch loss: 2.449, epoch iou: 0.101,  time cost: 11.938 s\n","torch.Size([786432, 8])\n","torch.Size([786432])\n","[Epoch-0 Iter-18] LR: 0.0010: iter loss: 2.002, iter iou: 0.131, epoch loss: 2.424, epoch iou: 0.103,  time cost: 11.582 s\n","torch.Size([786432, 8])\n","torch.Size([786432])\n","[Epoch-0 Iter-19] LR: 0.0010: iter loss: 1.952, iter iou: 0.122, epoch loss: 2.399, epoch iou: 0.104,  time cost: 12.778 s\n","torch.Size([786432, 8])\n","torch.Size([786432])\n","[Epoch-0 Iter-20] LR: 0.0010: iter loss: 1.955, iter iou: 0.119, epoch loss: 2.377, epoch iou: 0.104,  time cost: 12.811 s\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.figure(figsize=(10, 5))\n","plt.suptitle('dice loss + cross-entropy training')\n","plt.subplot(1, 2, 1)\n","plt.plot(loss_plot)\n","plt.ylabel('loss')\n","plt.xlabel('iteration')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(iou_plot)\n","plt.ylabel('Image-IoU (%)')\n","plt.xlabel('iteration')"],"metadata":{"id":"qE_1dpNRcsVY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(net.state_dict(), \"result.pt\")"],"metadata":{"id":"4SzUTrZYZzMZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(net.state_dict(), \n","                    pjoin(cfg.WEIGHTS_SAVE_ROOT, \"weights_ep_%d_%.3f_%.3f.pth\" \n","                            % (epoch, epoch_loss / epoch_size, epoch_miou / epoch_size)))"],"metadata":{"id":"OQmsLOcbaiXL"},"execution_count":null,"outputs":[]}]}