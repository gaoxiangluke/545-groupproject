{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bce+dice.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"c1ca17c89d3f46dfbc6c37cb0eee3b0d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_92ff303f586c4df5b33e2226812f7e25","IPY_MODEL_aa2b4e922474469d9a0c56bd674192d7","IPY_MODEL_d6a6e740fedc4ed3b4f07580729ef705"],"layout":"IPY_MODEL_f71e3ed17df64fe19a5ac288940775d4"}},"92ff303f586c4df5b33e2226812f7e25":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4f7712e6db9400db6ec0fc897d59fed","placeholder":"​","style":"IPY_MODEL_25268e5a500e4959a0a53e8d21cbb8ef","value":"100%"}},"aa2b4e922474469d9a0c56bd674192d7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0cf77a1897d4a309fd6445b5db74b21","max":178793939,"min":0,"orientation":"horizontal","style":"IPY_MODEL_96d9b367add44dc68f6f14468094e68a","value":178793939}},"d6a6e740fedc4ed3b4f07580729ef705":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b5d6a3809804d4d9912cd1bc37e0003","placeholder":"​","style":"IPY_MODEL_a9125d5ec1494604bea27b3443bf1491","value":" 171M/171M [00:01&lt;00:00, 112MB/s]"}},"f71e3ed17df64fe19a5ac288940775d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4f7712e6db9400db6ec0fc897d59fed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25268e5a500e4959a0a53e8d21cbb8ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0cf77a1897d4a309fd6445b5db74b21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96d9b367add44dc68f6f14468094e68a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3b5d6a3809804d4d9912cd1bc37e0003":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9125d5ec1494604bea27b3443bf1491":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17563,"status":"ok","timestamp":1649878595679,"user":{"displayName":"Xiang Gao","userId":"03678382663657440323"},"user_tz":240},"id":"hV3OfnRxglel","outputId":"a55fa767-b62b-4bca-d870-243d6fa9ba73"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/Shareddrives/EECS 545 Project/5DE/lane-detection-2019-howard')"],"metadata":{"id":"sM6qOXiJK2hf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torch.nn.functional as F\n","class UNetFactory(nn.Module):\n","    \"\"\"\n","    本质上就是一个U型的网络，先encode，后decode，中间可能有架bridge。\n","    其中encoder需要输出skip到decode那边做concatenate，使得decode阶段能补充信息。\n","    bridge不能存在下采样和上采样的操作。\n","    \"\"\"\n","    def __init__(self, encoder_blocks, decoder_blocks, bridge=None):\n","        super(UNetFactory, self).__init__()\n","        self.encoder = UNetEncoder(encoder_blocks)\n","        self.bridge = bridge\n","        self.decoder = UNetDecoder(decoder_blocks)\n","\n","    def forward(self, x):\n","        res = self.encoder(x)\n","        out, skips = res[0], res[1:]\n","        if self.bridge is not None:\n","            out = self.bridge(out)\n","        out = self.decoder(out, skips)\n","        return out\n","\n","class UNetEncoder(nn.Module):\n","    \"\"\"\n","    encoder会有多次下采样，下采样前的feature map要作为skip缓存起来将来送到decoder用。\n","    这里约定，以下采样为界线，将encoder分成多个block，其中第一个block无下采样操作，后面的每个block内都\n","    含有一次下采样操作。\n","    \"\"\"\n","    def __init__(self, blocks):\n","        super(UNetEncoder, self).__init__()\n","        assert len(blocks) > 0\n","        self.blocks = nn.ModuleList(blocks)\n","\n","    def forward(self, x):\n","        skips = []\n","        for i in range(len(self.blocks) - 1):\n","            x = self.blocks[i](x)\n","            skips.append(x)\n","        res = [self.blocks[i+1](x)]\n","        res += skips\n","        return res # 只能以这种方式返回多个tensor\n","\n","class UNetDecoder(nn.Module):\n","    \"\"\"\n","    decoder会有多次上采样，每次上采样后，要跟相应的skip做concatenate。\n","    这里约定，以上采样为界线，将decoder分成多个block，其中最后一个block无上采样操作，其他block内\n","    都含有一次上采样。如此一来，除第一个block以外，其他block都先做concatenate。\n","    \"\"\"\n","    def __init__(self, blocks):\n","        super(UNetDecoder, self).__init__()\n","        assert len(blocks) > 1\n","        self.blocks = nn.ModuleList(blocks)\n","    \n","    def _center_crop(self, skip, x):\n","        \"\"\"\n","        skip和x，谁比较大，就裁剪谁\n","        \"\"\"\n","        _, _, h1, w1 = skip.shape\n","        _, _, h2, w2 = x.shape\n","        ht, wt = min(h1, h2), min(w1, w2)\n","        dh1 = (h1 - ht) // 2 if h1 > ht else 0\n","        dw1 = (w1 - wt) // 2 if w1 > wt else 0\n","        dh2 = (h2 - ht) // 2 if h2 > ht else 0\n","        dw2 = (w2 - wt) // 2 if w2 > wt else 0\n","        return skip[:, :, dh1: (dh1 + ht), dw1: (dw1 + wt)], \\\n","                x[:, :, dh2: (dh2 + ht), dw2: (dw2 + wt)]\n","\n","    def forward(self, x, skips, reverse_skips=True):\n","        assert len(skips) == len(self.blocks) - 1\n","        if reverse_skips:\n","            skips = skips[::-1]\n","        x = self.blocks[0](x)\n","        for i in range(1, len(self.blocks)):\n","            skip, x = self._center_crop(skips[i-1], x)\n","            x = torch.cat([skip, x], dim=1)\n","            x = self.blocks[i](x)\n","        return x\n","\n","def unet_convs(in_channels, out_channels, padding=0):\n","    \"\"\"\n","    unet论文里出现次数最多的2个conv3x3(non-padding)的结构\n","    \"\"\"\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=padding, bias=False),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=padding, bias=False),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True),\n","    )\n","\n","\n","def unet(in_channels, out_channels):\n","    \"\"\"\n","    构造跟论文一致的unet网络\n","    https://arxiv.org/abs/1505.04597\n","    \"\"\"\n","    # encoder\n","    encoder_blocks = [\n","        # two conv3x3\n","        unet_convs(in_channels, 64),\n","        # max pool 2x2, two conv3x3\n","        nn.Sequential(\n","            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n","            unet_convs(64, 128)\n","        ),\n","        # max pool 2x2, two conv3x3\n","        nn.Sequential(\n","            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n","            unet_convs(128, 256)\n","        ),\n","        # max pool 2x2, two conv3x3\n","        nn.Sequential(\n","            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n","            unet_convs(256, 512)\n","        ),\n","        # max pool 2x2\n","        nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n","    ]\n","    # bridge\n","    bridge = nn.Sequential(\n","        # two conv3x3\n","        unet_convs(512, 1024)\n","    )\n","    # decoder\n","    decoder_blocks = [\n","        # up-conv2x2\n","        nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2),\n","        # two conv3x3, up-conv2x2\n","        nn.Sequential(\n","            unet_convs(1024, 512),\n","            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n","        ),\n","        # two conv3x3, up-conv2x2\n","        nn.Sequential(\n","            unet_convs(512, 256),\n","            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n","        ),\n","        # two conv3x3, up-conv2x2\n","        nn.Sequential(\n","            unet_convs(256, 128),\n","            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n","        ),\n","        # two conv3x3, conv1x1\n","        nn.Sequential(\n","            unet_convs(128, 64),\n","            nn.Conv2d(64, out_channels, kernel_size=1)\n","        )\n","    ]\n","    return UNetFactory(encoder_blocks, decoder_blocks, bridge)\n","\n","def unet_resnet(resnet_type, in_channels, out_channels, pretrained=True):\n","    \"\"\"\n","    利用resnet作为encoder，相应地，decoder也做一些改动，使得输出的尺寸跟原始的一致\n","    \"\"\"\n","    if resnet_type == 'resnet18':\n","        resnet = torchvision.models.resnet.resnet18(pretrained)\n","        encoder_out_channels = [in_channels, 64, 64, 128, 256, 512]  # encoder各个block的输出channel\n","    elif resnet_type == 'resnet34':\n","        resnet = torchvision.models.resnet.resnet34(pretrained)\n","        encoder_out_channels = [in_channels, 64, 64, 128, 256, 512]\n","    elif resnet_type == 'resnet50':\n","        resnet = torchvision.models.resnet.resnet50(pretrained)\n","        encoder_out_channels = [in_channels, 64, 256, 512, 1024, 2048]\n","    elif resnet_type == 'resnet101':\n","        resnet = torchvision.models.resnet.resnet101(pretrained)\n","        encoder_out_channels = [in_channels, 64, 256, 512, 1024, 2048]\n","    elif resnet_type == 'resnet152':\n","        resnet = torchvision.models.resnet.resnet152(pretrained)\n","        encoder_out_channels = [in_channels, 64, 256, 512, 1024, 2048]\n","    elif resnet_type == 'resnext50_32x4d':\n","        resnet = torchvision.models.resnet.resnext50_32x4d(pretrained)\n","        encoder_out_channels = [in_channels, 64, 256, 512, 1024, 2048]\n","    else:\n","        raise ValueError(\"unexpected resnet_type\")\n","\n","    # encoder\n","    encoder_blocks = [\n","        # org input\n","        nn.Sequential(),\n","        # conv1\n","        nn.Sequential(\n","            resnet.conv1,\n","            resnet.bn1,\n","            resnet.relu\n","        ),\n","        # conv2_x\n","        nn.Sequential(\n","            resnet.maxpool,\n","            resnet.layer1\n","        ),\n","        # conv3_x\n","        resnet.layer2,\n","        # conv4_x\n","        resnet.layer3,\n","        # conv5_x\n","        resnet.layer4\n","    ]\n","    # bridge\n","    bridge = None  # 感觉并无必要\n","    # decoder\n","    decoder_blocks = []\n","    in_ch = encoder_out_channels[-1]\n","    out_ch = in_ch // 2\n","    decoder_blocks.append(nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)) # up-conv2x2\n","    for i in range(1, len(encoder_blocks)-1):\n","        in_ch = encoder_out_channels[-i-1] + out_ch  # cat\n","        decoder_blocks.append(nn.Sequential(  # two conv3x3, up-conv2x2\n","            unet_convs(in_ch, out_ch, padding=1),\n","            nn.ConvTranspose2d(out_ch, out_ch//2, kernel_size=2, stride=2),\n","        ))\n","        out_ch = out_ch // 2\n","    in_ch = encoder_out_channels[0] + out_ch  # cat\n","    decoder_blocks.append(nn.Sequential(  # two conv3x3, conv1x1\n","        unet_convs(in_ch, out_ch, padding=1),\n","        nn.Conv2d(out_ch, out_channels, kernel_size=1)\n","    ))\n","\n","    return UNetFactory(encoder_blocks, decoder_blocks, bridge)"],"metadata":{"id":"MDTNSK_bQCqL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"rUbpIzPCRmNE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","@description: Configure Class \n","\"\"\"\n","\n","from os.path import join as pjoin\n","from os.path import dirname, abspath\n","import torch\n","\n","class ConfigTrain(object):\n","    # 目录\n","    PROJECT_ROOT = \"/content/drive/Shareddrives/EECS 545 Project/5DE/lane-detection-2019-howard\"\n","    DATA_LIST_ROOT = pjoin(PROJECT_ROOT, 'data_list')\n","    TRAIN_ROOT = \"/content/drive/Shareddrives/EECS 545 Project/data\"\n","    IMAGE_ROOT = pjoin(TRAIN_ROOT, 'Image_Data')\n","    LABEL_ROOT = pjoin(TRAIN_ROOT, 'Gray_Label')\n","    WEIGHTS_ROOT = pjoin(PROJECT_ROOT, 'weights')\n","    WEIGHTS_SAVE_ROOT = pjoin(WEIGHTS_ROOT, '1536x512_b2_unet_resnext50_32x4d')\n","    LOG_ROOT = pjoin(PROJECT_ROOT, 'logs')\n","\n","    # log文件\n","    # LOG_SUSPICIOUS_FILES = pjoin(LOG_ROOT, 'suspicious_files.log')\n","    # LOG_SUSPICIOUS_FILES = pjoin(LOG_ROOT, 'suspicious_files_b1.log')\n","    # LOG_SUSPICIOUS_FILES = pjoin(LOG_ROOT, 'suspicious_files_b4.log')\n","    LOG_SUSPICIOUS_FILES = pjoin(LOG_ROOT, 'suspicious_files_b2.log')\n","\n","    # 设备\n","    DEVICE = 'cuda:0' \n","    \n","    if torch.cuda.is_available():\n","      print(\"Using the GPU. You are good to go!\")\n","      DEVICE = 'cuda'\n","    else:\n","      print(\"Using the CPU. Overall speed may be slowed down\")\n","      DEVICE = 'cpu'\n","    \n","    # 网络类型\n","    NET_NAME = 'unet_resnet101'\n","    #NET_NAME = 'resnext50_32x4d'\n","    # 网络参数\n","    NUM_CLASSES = 8  # 8个类别\n","    IMAGE_SIZE = (768, 256)  # 训练的图片的尺寸(h,w)\n","    # IMAGE_SIZE = (1024, 384)  # 训练的图片的尺寸(h,w)\n","    #IMAGE_SIZE = (1536, 512)  # 训练的图片的尺寸(h,w)\n","    HEIGHT_CROP_OFFSET = 690  # 在height方向上将原图裁掉的offset\n","    # BATCH_SIZE = 8  # 数据批次大小\n","    # BATCH_SIZE = 1  # 数据批次大小\n","    # BATCH_SIZE = 4  # 数据批次大小\n","    BATCH_SIZE = 4  # 数据批次大小\n","    EPOCH_NUM = 4  # 总轮次\n","    PRETRAIN = False # 是否加载预训练的权重\n","    EPOCH_BEGIN = 0  # 接着前面的epoch训练，默认0，表示从头训练\n","    PRETRAINED_WEIGHTS = pjoin(WEIGHTS_ROOT, '1024x384_b4_unet_resnext50_32x4d', 'result_6.pt')\n","    BASE_LR = 0.001  # 学习率\n","    LR_STRATEGY = [\n","        [0.001], # epoch 0\n","        [0.001], # epoch 1\n","        [0.001], # epoch 2\n","        [0.001, 0.0006, 0.0003, 0.0001, 0.0004, 0.0008, 0.001], # epoch 3\n","        [0.001, 0.0006, 0.0003, 0.0001, 0.0004, 0.0008, 0.001], # epoch 4\n","        [0.001, 0.0006, 0.0003, 0.0001, 0.0004, 0.0008, 0.001], # epoch 5\n","        [0.0004, 0.0003, 0.0002, 0.0001, 0.0002, 0.0003, 0.0004], # epoch 6\n","        [0.0004, 0.0003, 0.0002, 0.0001, 0.0002, 0.0003, 0.0004], # epoch 7\n","    ]\n","    SUSPICIOUS_RATE = 0.8  # 可疑比例：当某个iteration的miou比当前epoch_miou的可疑比例还要小的时候，记录此次iteration的训练数据索引，人工排查是否数据有问题\n","\n","## TO DO: Define PROJECT_ROOT##\n","'''    \n","class ConfigInference(object):\n","    # 目录\n","    PROJECT_ROOT = dirname(abspath(__file__)) \n","    DATA_ROOT = pjoin(PROJECT_ROOT, 'data')\n","    IMAGE_ROOT = pjoin(DATA_ROOT, 'TestImage')\n","    LABEL_ROOT = pjoin(DATA_ROOT, 'TestLabel')\n","    OVERLAY_ROOT = pjoin(DATA_ROOT, 'TestOverlay')\n","    WEIGHTS_ROOT = pjoin(PROJECT_ROOT, 'weights')\n","    PRETRAINED_WEIGHTS = pjoin(WEIGHTS_ROOT, '1024x384_b4_unet_resnext50_32x4d', 'resnext50_32x4d-7cdf4587.pth')\n","    LOG_ROOT = pjoin(PROJECT_ROOT, 'logs')\n","\n","    # 设备\n","    DEVICE = 'cuda:0'\n","\n","    # 网络类型\n","    NET_NAME = 'resnext50_32x4d'\n","\n","    # 网络参数\n","    NUM_CLASSES = 8  # 8个类别\n","    # IMAGE_SIZE = (768, 256)  # 训练的图片的尺寸(h,w)\n","    IMAGE_SIZE = (1024, 384)  # 训练的图片的尺寸(h,w)\n","    # IMAGE_SIZE = (1536, 512)  # 训练的图片的尺寸(h,w)\n","    HEIGHT_CROP_OFFSET = 690  # 在height方向上将原图裁掉的offset\n","    BATCH_SIZE = 1  # 数据批次大小\n","\n","    # 原图的大小\n","    IMAGE_SIZE_ORG = (3384, 1710)\n","'''    "],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":165},"id":"dha6U_LzGxtJ","executionInfo":{"status":"ok","timestamp":1649878603687,"user_tz":240,"elapsed":24,"user":{"displayName":"Xiang Gao","userId":"03678382663657440323"}},"outputId":"c760a492-68c8-422e-c491-5e2f3be1d5ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using the GPU. You are good to go!\n"]},{"output_type":"execute_result","data":{"text/plain":["\"    \\nclass ConfigInference(object):\\n    # 目录\\n    PROJECT_ROOT = dirname(abspath(__file__)) \\n    DATA_ROOT = pjoin(PROJECT_ROOT, 'data')\\n    IMAGE_ROOT = pjoin(DATA_ROOT, 'TestImage')\\n    LABEL_ROOT = pjoin(DATA_ROOT, 'TestLabel')\\n    OVERLAY_ROOT = pjoin(DATA_ROOT, 'TestOverlay')\\n    WEIGHTS_ROOT = pjoin(PROJECT_ROOT, 'weights')\\n    PRETRAINED_WEIGHTS = pjoin(WEIGHTS_ROOT, '1024x384_b4_unet_resnext50_32x4d', 'resnext50_32x4d-7cdf4587.pth')\\n    LOG_ROOT = pjoin(PROJECT_ROOT, 'logs')\\n\\n    # 设备\\n    DEVICE = 'cuda:0'\\n\\n    # 网络类型\\n    NET_NAME = 'resnext50_32x4d'\\n\\n    # 网络参数\\n    NUM_CLASSES = 8  # 8个类别\\n    # IMAGE_SIZE = (768, 256)  # 训练的图片的尺寸(h,w)\\n    IMAGE_SIZE = (1024, 384)  # 训练的图片的尺寸(h,w)\\n    # IMAGE_SIZE = (1536, 512)  # 训练的图片的尺寸(h,w)\\n    HEIGHT_CROP_OFFSET = 690  # 在height方向上将原图裁掉的offset\\n    BATCH_SIZE = 1  # 数据批次大小\\n\\n    # 原图的大小\\n    IMAGE_SIZE_ORG = (3384, 1710)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["'''\n","Define loss\n","'''\n","class MySoftmaxCrossEntropyLoss(nn.Module):\n","\n","    def __init__(self, nbclasses):\n","        super(MySoftmaxCrossEntropyLoss, self).__init__()\n","        self.nbclasses = nbclasses\n","\n","    def forward(self, inputs, target):\n","        if inputs.dim() > 2:\n","            inputs = inputs.view(inputs.size(0), inputs.size(1), -1)  # N,C,H,W => N,C,H*W\n","            inputs = inputs.transpose(1, 2)  # N,C,H*W => N,H*W,C\n","            inputs = inputs.contiguous().view(-1, self.nbclasses)  # N,H*W,C => N*H*W,C\n","        target = target.view(-1)\n","        return nn.CrossEntropyLoss(reduction=\"mean\")(inputs, target)\n","\n","\n","class FocalLoss(nn.Module):\n","\n","    def __init__(self, gamma=0, alpha=None, size_average=True):\n","        super(FocalLoss, self).__init__()\n","        self.gamma = gamma\n","        self.alpha = alpha\n","        self.alpha = torch.tensor([alpha, 1 - alpha])\n","        self.size_average = size_average\n","\n","    def forward(self, inputs, target):\n","        if inputs.dim() > 2:\n","            inputs = inputs\n","            inputs = inputs.view(inputs.size(0), inputs.size(1), -1)  # N,C,H,W => N,C,H*W\n","            inputs = inputs.transpose(1, 2)  # N,C,H*W => N,H*W,C\n","            inputs = inputs.contiguous().view(-1, inputs.size(2))  # N,H*W,C => N*H*W,C\n","        target = target.view(-1, 1)\n","\n","        logpt = F.log_softmax(inputs,dim=1)\n","        logpt = logpt.gather(1, target)\n","        logpt = logpt.view(-1)\n","        pt = logpt.exp()\n","\n","        if self.alpha is not None:\n","            if self.alpha.type() != inputs.data.type():\n","                self.alpha = self.alpha.type_as(inputs.data)\n","            at = self.alpha.gather(0, target.view(-1))\n","            logpt = logpt * at\n","        # mask = mask.view(-1)\n","        loss = -1 * (1 - pt) ** self.gamma * logpt #* mask\n","        if self.size_average:\n","            return loss.mean()\n","        else:\n","            return loss.sum()\n","\n","\n","def make_one_hot(input, num_classes):\n","    \"\"\"Convert class index tensor to one hot encoding tensor.\n","    Args:\n","         input: A tensor of shape [N, 1, *]\n","         num_classes: An int of number of class\n","    Returns:\n","        A tensor of shape [N, num_classes, *]\n","    \"\"\"\n","    shape = np.array(input.shape)\n","    shape[1] = num_classes\n","    shape = tuple(shape)\n","    result = torch.zeros(shape)\n","    result = result.scatter_(1, input.cpu(), 1)\n","\n","    return result\n","\n","\n","class BinaryDiceLoss(nn.Module):\n","    \"\"\"Dice loss of binary class\n","    Args:\n","        smooth: A float number to smooth loss, and avoid NaN error, default: 1\n","        p: Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2\n","        predict: A tensor of shape [N, *]\n","        target: A tensor of shape same with predict\n","        reduction: Reduction method to apply, return mean over batch if 'mean',\n","            return sum if 'sum', return a tensor of shape [N,] if 'none'\n","    Returns:\n","        Loss tensor according to arg reduction\n","    Raise:\n","        Exception if unexpected reduction\n","    \"\"\"\n","    def __init__(self, smooth=1, p=2, reduction='mean'):\n","        super(BinaryDiceLoss, self).__init__()\n","        self.smooth = smooth\n","        self.p = p\n","        self.reduction = reduction\n","\n","    def forward(self, predict, target):\n","        assert predict.shape[0] == target.shape[0], \"predict & target batch size don't match\"\n","        predict = predict.contiguous().view(predict.shape[0], -1)\n","        target = target.contiguous().view(target.shape[0], -1)\n","        num = 2*torch.sum(torch.mul(predict, target), dim=1) + self.smooth\n","        den = torch.sum(predict.pow(self.p) + target.pow(self.p), dim=1) + self.smooth\n","\n","        loss = 1 - num / den\n","\n","        if self.reduction == 'mean':\n","            return loss.mean()\n","        elif self.reduction == 'sum':\n","            return loss.sum()\n","        elif self.reduction == 'none':\n","            return loss\n","        else:\n","            raise Exception('Unexpected reduction {}'.format(self.reduction))\n","\n","\n","class DiceLoss(nn.Module):\n","    \"\"\"Dice loss, need one hot encode input\n","    Args:\n","        weight: An array of shape [num_classes,]\n","        ignore_index: class index to ignore\n","        predict: A tensor of shape [N, C, *]\n","        target: A tensor of same shape with predict\n","        other args pass to BinaryDiceLoss\n","    Return:\n","        same as BinaryDiceLoss\n","    \"\"\"\n","    def __init__(self, weight=None, ignore_index=None, **kwargs):\n","        super(DiceLoss, self).__init__()\n","        self.kwargs = kwargs\n","        self.weight = weight\n","        self.ignore_index = ignore_index\n","\n","    def forward(self, predict, target):\n","        assert predict.shape == target.shape, 'predict & target shape do not match'\n","        dice = BinaryDiceLoss(**self.kwargs)\n","        total_loss = 0\n","        predict = F.softmax(predict, dim=1)\n","\n","        for i in range(target.shape[1]):\n","            if i != self.ignore_index:\n","                dice_loss = dice(predict[:, i], target[:, i])\n","                if self.weight is not None:\n","                    assert self.weight.shape[0] == target.shape[1], \\\n","                        'Expect weight shape [{}], get[{}]'.format(target.shape[1], self.weight.shape[0])\n","                    dice_loss *= self.weights[i]\n","                total_loss += dice_loss\n","\n","        return total_loss/target.shape[1]\n","\n"],"metadata":{"id":"VhCWX0QNWYYI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\"\"\"\n","Lovasz-Softmax and Jaccard hinge loss in PyTorch\n","Maxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n","\"\"\"\n","\n","from __future__ import print_function, division\n","\n","import torch\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","import numpy as np\n","try:\n","    from itertools import  ifilterfalse\n","except ImportError: # py3k\n","    from itertools import  filterfalse as ifilterfalse\n","\n","\n","def lovasz_grad(gt_sorted):\n","    \"\"\"\n","    Computes gradient of the Lovasz extension w.r.t sorted errors\n","    See Alg. 1 in paper\n","    \"\"\"\n","    p = len(gt_sorted)\n","    gts = gt_sorted.sum()\n","    intersection = gts - gt_sorted.float().cumsum(0)\n","    union = gts + (1 - gt_sorted).float().cumsum(0)\n","    jaccard = 1. - intersection / union\n","    if p > 1: # cover 1-pixel case\n","        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n","    return jaccard\n","\n","\n","def iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n","    \"\"\"\n","    IoU for foreground class\n","    binary: 1 foreground, 0 background\n","    \"\"\"\n","    if not per_image:\n","        preds, labels = (preds,), (labels,)\n","    ious = []\n","    for pred, label in zip(preds, labels):\n","        intersection = ((label == 1) & (pred == 1)).sum()\n","        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n","        if not union:\n","            iou = EMPTY\n","        else:\n","            iou = float(intersection) / float(union)\n","        ious.append(iou)\n","    iou = mean(ious)    # mean accross images if per_image\n","    return 100 * iou\n","\n","\n","def iou(preds, labels, C, EMPTY=1., ignore=None, per_image=False):\n","    \"\"\"\n","    Array of IoU for each (non ignored) class\n","    \"\"\"\n","    if not per_image:\n","        preds, labels = (preds,), (labels,)\n","    ious = []\n","    for pred, label in zip(preds, labels):\n","        iou = []    \n","        for i in range(C):\n","            if i != ignore: # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n","                intersection = ((label == i) & (pred == i)).sum()\n","                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n","                if not union:\n","                    iou.append(EMPTY)\n","                else:\n","                    iou.append(float(intersection) / float(union))\n","        ious.append(iou)\n","    ious = [mean(iou) for iou in zip(*ious)] # mean accross images if per_image\n","    return 100 * np.array(ious)\n","\n","\n","# --------------------------- BINARY LOSSES ---------------------------\n","\n","\n","def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n","    \"\"\"\n","    Binary Lovasz hinge loss\n","      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n","      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n","      per_image: compute the loss per image instead of per batch\n","      ignore: void class id\n","    \"\"\"\n","    if per_image:\n","        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n","                          for log, lab in zip(logits, labels))\n","    else:\n","        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n","    return loss\n","\n","\n","def lovasz_hinge_flat(logits, labels):\n","    \"\"\"\n","    Binary Lovasz hinge loss\n","      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n","      labels: [P] Tensor, binary ground truth labels (0 or 1)\n","      ignore: label to ignore\n","    \"\"\"\n","    if len(labels) == 0:\n","        # only void pixels, the gradients should be 0\n","        return logits.sum() * 0.\n","    signs = 2. * labels.float() - 1.\n","    errors = (1. - logits * Variable(signs))\n","    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n","    perm = perm.data\n","    gt_sorted = labels[perm]\n","    grad = lovasz_grad(gt_sorted)\n","    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n","    return loss\n","\n","\n","def flatten_binary_scores(scores, labels, ignore=None):\n","    \"\"\"\n","    Flattens predictions in the batch (binary case)\n","    Remove labels equal to 'ignore'\n","    \"\"\"\n","    scores = scores.view(-1)\n","    labels = labels.view(-1)\n","    if ignore is None:\n","        return scores, labels\n","    valid = (labels != ignore)\n","    vscores = scores[valid]\n","    vlabels = labels[valid]\n","    return vscores, vlabels\n","\n","\n","class StableBCELoss(torch.nn.modules.Module):\n","    def __init__(self):\n","         super(StableBCELoss, self).__init__()\n","    def forward(self, input, target):\n","         neg_abs = - input.abs()\n","         loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n","         return loss.mean()\n","\n","\n","def binary_xloss(logits, labels, ignore=None):\n","    \"\"\"\n","    Binary Cross entropy loss\n","      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n","      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n","      ignore: void class id\n","    \"\"\"\n","    logits, labels = flatten_binary_scores(logits, labels, ignore)\n","    loss = StableBCELoss()(logits, Variable(labels.float()))\n","    return loss\n","\n","\n","# --------------------------- MULTICLASS LOSSES ---------------------------\n","\n","\n","def lovasz_softmax(probas, labels, classes='present', per_image=False, ignore=None):\n","    \"\"\"\n","    Multi-class Lovasz-Softmax loss\n","      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1).\n","              Interpreted as binary (sigmoid) output with outputs of size [B, H, W].\n","      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n","      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n","      per_image: compute the loss per image instead of per batch\n","      ignore: void class labels\n","    \"\"\"\n","    if per_image:\n","        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), classes=classes)\n","                          for prob, lab in zip(probas, labels))\n","    else:\n","        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), classes=classes)\n","    return loss\n","\n","\n","def lovasz_softmax_flat(probas, labels, classes='present'):\n","    \"\"\"\n","    Multi-class Lovasz-Softmax loss\n","      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n","      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n","      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n","    \"\"\"\n","    if probas.numel() == 0:\n","        # only void pixels, the gradients should be 0\n","        return probas * 0.\n","    C = probas.size(1)\n","    losses = []\n","    class_to_sum = list(range(C)) if classes in ['all', 'present'] else classes\n","    for c in class_to_sum:\n","        fg = (labels == c).float() # foreground for class c\n","        if (classes is 'present' and fg.sum() == 0):\n","            continue\n","        if C == 1:\n","            if len(classes) > 1:\n","                raise ValueError('Sigmoid output possible only with 1 class')\n","            class_pred = probas[:, 0]\n","        else:\n","            class_pred = probas[:, c]\n","        errors = (Variable(fg) - class_pred).abs()\n","        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n","        perm = perm.data\n","        fg_sorted = fg[perm]\n","        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n","    return mean(losses)\n","\n","\n","def flatten_probas(probas, labels, ignore=None):\n","    \"\"\"\n","    Flattens predictions in the batch\n","    \"\"\"\n","    if probas.dim() == 3:\n","        # assumes output of a sigmoid layer\n","        B, H, W = probas.size()\n","        probas = probas.view(B, 1, H, W)\n","    B, C, H, W = probas.size()\n","    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n","    labels = labels.view(-1)\n","    if ignore is None:\n","        return probas, labels\n","    valid = (labels != ignore)\n","    vprobas = probas[valid.nonzero().squeeze()]\n","    vlabels = labels[valid]\n","    return vprobas, vlabels\n","\n","def xloss(logits, labels, ignore=None):\n","    \"\"\"\n","    Cross entropy loss\n","    \"\"\"\n","    return F.cross_entropy(logits, Variable(labels), ignore_index=255)\n","\n","\n","# --------------------------- HELPER FUNCTIONS ---------------------------\n","def isnan(x):\n","    return x != x\n","    \n","    \n","def mean(l, ignore_nan=False, empty=0):\n","    \"\"\"\n","    nanmean compatible with generators.\n","    \"\"\"\n","    l = iter(l)\n","    if ignore_nan:\n","        l = ifilterfalse(isnan, l)\n","    try:\n","        n = 1\n","        acc = next(l)\n","    except StopIteration:\n","        if empty == 'raise':\n","            raise ValueError('Empty mean')\n","        return empty\n","    for n, v in enumerate(l, 2):\n","        acc += v\n","    if n == 1:\n","        return acc\n","    return acc / n"],"metadata":{"id":"EAFofjy4ekge"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","util files\n","'''\n","def create_net(in_channels, out_channels, net_name='unet'):\n","    \"\"\"\n","    创建网络\n","    :param in_channels: 输入通道数\n","    :param out_channels: 输出通道数\n","    # :param net_name: 网络类型，可选 unet | unet_resnet18/34/50/101/152 |unet_resnext50_32x4d | deeplabv3p\n","    :param net_name: 网络类型，可选 unet | unet_resnet34\n","    \"\"\"\n","    if net_name == 'unet':\n","        net = unet(in_channels, out_channels)\n","    elif net_name == 'unet_resnet34':\n","        net = unet_resnet('resnet34', in_channels, out_channels)\n","    elif net_name == 'unet_resnet50':\n","        net = unet_resnet('resnet50', in_channels, out_channels)\n","    elif net_name == 'unet_resnet101':\n","        net = unet_resnet('resnet101', in_channels, out_channels)    \n","    elif net_name == 'resnext50_32x4d':\n","        net = unet_resnet('resnext50_32x4d', in_channels, out_channels)\n","    else:\n","        raise ValueError('Not supported net_name: {}'.format(net_name))\n","\n","    return net\n","\n","def create_loss(predicts: torch.Tensor, labels: torch.Tensor, num_classes):\n","    \"\"\"\n","    创建loss\n","    @param predicts: shape=(n, c, h, w)\n","    @param labels: shape=(n, h, w) or shape=(n, 1, h, w)\n","    @param num_classes: int should equal to channels of predicts\n","    @return: loss, mean_iou\n","    \"\"\"\n","    # permute to (n, h, w, c)\n","    predicts = predicts.permute((0, 2, 3, 1))\n","    # reshape to (-1, num_classes)  每个像素在每种分类上都有一个概率\n","    predicts = predicts.reshape((-1, num_classes))\n","    ##print(predicts.shape)\n","    ##print(labels.flatten().shape)\n","    # BCE with DICE\n","    bce_loss = F.cross_entropy(predicts, labels.flatten(), reduction='mean')  # 函数内会自动做softmax\n","    # 将labels做one_hot处理，得到的形状跟predicts相同\n","    labels_one_hot = utils.make_one_hot(labels.reshape((-1, 1)), num_classes)\n","    dice_loss = utils.DiceLoss()(predicts, labels_one_hot.to(labels.device))  # torch没有原生的，从老师给的代码里拿过来用\n","    loss = bce_loss + dice_loss\n","    \n","    ious = compute_iou(predicts, labels.reshape((-1, 1)), num_classes)\n","    return loss, torch.mean(ious)\n","\n","def compute_iou(predicts, labels, num_classes):\n","    \"\"\"\n","    计算iou\n","    @param predicts: shape=(-1, classes)\n","    @param labels: shape=(-1, 1)\n","    \"\"\"\n","    ious = torch.zeros(num_classes)\n","    predicts = F.softmax(predicts, dim=1)\n","    predicts = torch.argmax(predicts, dim=1, keepdim=True)\n","    for i in range(num_classes):\n","        intersect = torch.sum((predicts == i) * (labels == i))\n","        area = torch.sum(predicts == i) + torch.sum(labels == i) - intersect\n","        ious[i] = intersect / (area + 1e-6)\n","    return ious\n"],"metadata":{"id":"b7tOt0NiQTyj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","@description: 执行训练\n","\"\"\"\n","\n","\n","\"\"\"\n","import\n","\"\"\"\n","#from config import ConfigTrain\n","import utils\n","from os.path import join as pjoin\n","import pandas as pd\n","import numpy as np\n","import cv2\n","import torch\n","import time\n","\n","\"\"\"\n","main\n","\"\"\"\n","if __name__ == '__main__':\n","    cfg = ConfigTrain()\n","    print('Pick device: ', cfg.DEVICE)\n","    device = torch.device(cfg.DEVICE)\n","\n","    # 网络\n","    print('Generating net: ', cfg.NET_NAME)\n","    net = create_net(3, cfg.NUM_CLASSES, net_name=cfg.NET_NAME)\n","    if cfg.PRETRAIN:  # 加载预训练权重\n","        print('Load pretrain weights: ', cfg.PRETRAINED_WEIGHTS)\n","        net.load_state_dict(torch.load(cfg.PRETRAINED_WEIGHTS, map_location='cpu'))\n","    net.to(device)\n","    # 优化器\n","    optimizer = torch.optim.Adam(net.parameters(), lr=cfg.BASE_LR) \n","\n","    # 训练数据生成器\n","    print('Preparing data... batch_size: {}, image_size: {}, crop_offset: {}'.format(cfg.BATCH_SIZE, cfg.IMAGE_SIZE, cfg.HEIGHT_CROP_OFFSET))\n","    df_train = pd.read_csv(pjoin(cfg.DATA_LIST_ROOT, 'train.csv'))\n","    data_generator = utils.train_data_generator(np.array(df_train['image']),\n","                                                np.array(df_train['label']),\n","                                                cfg.BATCH_SIZE, cfg.IMAGE_SIZE, cfg.HEIGHT_CROP_OFFSET)\n","\n","    # 训练\n","    print('Let us train ...')\n","    log_iters = 1  # 多少次迭代打印一次log\n","    epoch_size = int(len(df_train) / cfg.BATCH_SIZE)  # 一个轮次包含的迭代次数\n","    ##trn_loss_hist = []\n","    ##iou_hist = []\n","    loss_plot = []\n","    iou_plot = []\n","    for epoch in range(cfg.EPOCH_BEGIN, cfg.EPOCH_NUM):\n","        epoch_loss = 0.0\n","        epoch_miou = 0.0\n","        last_epoch_miou = 0.0\n","        prev_time = time.time()\n","        for iteration in range(1 , epoch_size + 1):\n","            images, labels, images_filename = next(data_generator)\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            lr = utils.ajust_learning_rate(optimizer, cfg.LR_STRATEGY, epoch, iteration-1, epoch_size)\n","\n","            predicts = net(images)\n","\n","            optimizer.zero_grad()\n","\n","            loss, mean_iou = create_loss(predicts, labels, cfg.NUM_CLASSES)\n","            ##trn_loss_hist.append(loss)\n","            ##iou_hist.append(mean_iou)\n","            epoch_loss += loss.item()\n","            epoch_miou += mean_iou.item()\n","\n","            print(\"[Epoch-%d Iter-%d] LR: %.4f: iter loss: %.3f, iter iou: %.3f, epoch loss: %.3f, epoch iou: %.3f,  time cost: %.3f s\"\n","                % (epoch, iteration, lr, loss.item(), mean_iou.item(), epoch_loss / iteration, epoch_miou / iteration, time.time() - prev_time))\n","            prev_time = time.time()\n","\n","            if mean_iou.item() < last_epoch_miou * cfg.SUSPICIOUS_RATE:\n","              ## TO DO: define log file or create a log file##\n","                with open(cfg.LOG_SUSPICIOUS_FILES, 'a+') as f:\n","                    for filename in images_filename:\n","                        f.write(\"{}\\n\".format(filename))\n","                    f.flush()\n","\n","            last_epoch_miou = epoch_miou / iteration\n","            loss_plot.append(loss.item())\n","            iou_plot.append(mean_iou.item())\n","            loss.backward()\n","            optimizer.step()\n","\n","        \n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["c1ca17c89d3f46dfbc6c37cb0eee3b0d","92ff303f586c4df5b33e2226812f7e25","aa2b4e922474469d9a0c56bd674192d7","d6a6e740fedc4ed3b4f07580729ef705","f71e3ed17df64fe19a5ac288940775d4","a4f7712e6db9400db6ec0fc897d59fed","25268e5a500e4959a0a53e8d21cbb8ef","c0cf77a1897d4a309fd6445b5db74b21","96d9b367add44dc68f6f14468094e68a","3b5d6a3809804d4d9912cd1bc37e0003","a9125d5ec1494604bea27b3443bf1491"]},"id":"gl-yCIdGOCQN","outputId":"345f6739-91b0-4c01-9d63-12f3efeb6fa8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pick device:  cuda\n","Generating net:  unet_resnet101\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/171M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1ca17c89d3f46dfbc6c37cb0eee3b0d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Preparing data... batch_size: 4, image_size: (768, 256), crop_offset: 690\n","Let us train ...\n","[Epoch-0 Iter-1] LR: 0.0010: iter loss: 1.909, iter iou: 0.048, epoch loss: 1.909, epoch iou: 0.048,  time cost: 27.151 s\n","[Epoch-0 Iter-2] LR: 0.0010: iter loss: 1.824, iter iou: 0.066, epoch loss: 1.867, epoch iou: 0.057,  time cost: 23.316 s\n","[Epoch-0 Iter-3] LR: 0.0010: iter loss: 1.733, iter iou: 0.099, epoch loss: 1.822, epoch iou: 0.071,  time cost: 22.225 s\n","[Epoch-0 Iter-4] LR: 0.0010: iter loss: 1.589, iter iou: 0.115, epoch loss: 1.764, epoch iou: 0.082,  time cost: 18.515 s\n","[Epoch-0 Iter-5] LR: 0.0010: iter loss: 1.481, iter iou: 0.119, epoch loss: 1.707, epoch iou: 0.089,  time cost: 9.737 s\n","[Epoch-0 Iter-6] LR: 0.0010: iter loss: 1.373, iter iou: 0.122, epoch loss: 1.651, epoch iou: 0.095,  time cost: 14.042 s\n","[Epoch-0 Iter-7] LR: 0.0010: iter loss: 1.288, iter iou: 0.118, epoch loss: 1.599, epoch iou: 0.098,  time cost: 10.212 s\n","[Epoch-0 Iter-8] LR: 0.0010: iter loss: 1.244, iter iou: 0.119, epoch loss: 1.555, epoch iou: 0.101,  time cost: 10.081 s\n","[Epoch-0 Iter-9] LR: 0.0010: iter loss: 1.146, iter iou: 0.121, epoch loss: 1.510, epoch iou: 0.103,  time cost: 11.231 s\n","[Epoch-0 Iter-10] LR: 0.0010: iter loss: 1.091, iter iou: 0.121, epoch loss: 1.468, epoch iou: 0.105,  time cost: 11.530 s\n","[Epoch-0 Iter-11] LR: 0.0010: iter loss: 1.032, iter iou: 0.122, epoch loss: 1.428, epoch iou: 0.106,  time cost: 10.207 s\n","[Epoch-0 Iter-12] LR: 0.0010: iter loss: 0.984, iter iou: 0.123, epoch loss: 1.391, epoch iou: 0.108,  time cost: 11.515 s\n","[Epoch-0 Iter-13] LR: 0.0010: iter loss: 0.946, iter iou: 0.122, epoch loss: 1.357, epoch iou: 0.109,  time cost: 10.028 s\n","[Epoch-0 Iter-14] LR: 0.0010: iter loss: 0.939, iter iou: 0.120, epoch loss: 1.327, epoch iou: 0.110,  time cost: 9.540 s\n","[Epoch-0 Iter-15] LR: 0.0010: iter loss: 0.866, iter iou: 0.123, epoch loss: 1.296, epoch iou: 0.111,  time cost: 9.559 s\n","[Epoch-0 Iter-16] LR: 0.0010: iter loss: 0.867, iter iou: 0.121, epoch loss: 1.269, epoch iou: 0.111,  time cost: 9.484 s\n","[Epoch-0 Iter-17] LR: 0.0010: iter loss: 0.806, iter iou: 0.123, epoch loss: 1.242, epoch iou: 0.112,  time cost: 10.099 s\n","[Epoch-0 Iter-18] LR: 0.0010: iter loss: 0.783, iter iou: 0.122, epoch loss: 1.217, epoch iou: 0.112,  time cost: 9.836 s\n","[Epoch-0 Iter-19] LR: 0.0010: iter loss: 0.806, iter iou: 0.119, epoch loss: 1.195, epoch iou: 0.113,  time cost: 10.189 s\n","[Epoch-0 Iter-20] LR: 0.0010: iter loss: 0.777, iter iou: 0.120, epoch loss: 1.174, epoch iou: 0.113,  time cost: 9.700 s\n","[Epoch-0 Iter-21] LR: 0.0010: iter loss: 0.709, iter iou: 0.122, epoch loss: 1.152, epoch iou: 0.114,  time cost: 9.717 s\n","[Epoch-0 Iter-22] LR: 0.0010: iter loss: 0.710, iter iou: 0.121, epoch loss: 1.132, epoch iou: 0.114,  time cost: 9.878 s\n","[Epoch-0 Iter-23] LR: 0.0010: iter loss: 0.677, iter iou: 0.122, epoch loss: 1.112, epoch iou: 0.114,  time cost: 10.925 s\n","[Epoch-0 Iter-24] LR: 0.0010: iter loss: 0.695, iter iou: 0.120, epoch loss: 1.095, epoch iou: 0.114,  time cost: 10.427 s\n","[Epoch-0 Iter-25] LR: 0.0010: iter loss: 0.717, iter iou: 0.118, epoch loss: 1.080, epoch iou: 0.115,  time cost: 9.579 s\n","[Epoch-0 Iter-26] LR: 0.0010: iter loss: 0.605, iter iou: 0.122, epoch loss: 1.061, epoch iou: 0.115,  time cost: 9.695 s\n","[Epoch-0 Iter-27] LR: 0.0010: iter loss: 0.606, iter iou: 0.121, epoch loss: 1.045, epoch iou: 0.115,  time cost: 9.751 s\n","[Epoch-0 Iter-28] LR: 0.0010: iter loss: 0.574, iter iou: 0.122, epoch loss: 1.028, epoch iou: 0.115,  time cost: 9.901 s\n","[Epoch-0 Iter-29] LR: 0.0010: iter loss: 0.529, iter iou: 0.123, epoch loss: 1.011, epoch iou: 0.116,  time cost: 9.738 s\n","[Epoch-0 Iter-30] LR: 0.0010: iter loss: 0.515, iter iou: 0.123, epoch loss: 0.994, epoch iou: 0.116,  time cost: 9.557 s\n","[Epoch-0 Iter-31] LR: 0.0010: iter loss: 0.517, iter iou: 0.122, epoch loss: 0.979, epoch iou: 0.116,  time cost: 9.790 s\n","[Epoch-0 Iter-32] LR: 0.0010: iter loss: 0.494, iter iou: 0.122, epoch loss: 0.963, epoch iou: 0.116,  time cost: 9.648 s\n","[Epoch-0 Iter-33] LR: 0.0010: iter loss: 0.465, iter iou: 0.123, epoch loss: 0.948, epoch iou: 0.116,  time cost: 10.137 s\n","[Epoch-0 Iter-34] LR: 0.0010: iter loss: 0.471, iter iou: 0.122, epoch loss: 0.934, epoch iou: 0.117,  time cost: 10.491 s\n","[Epoch-0 Iter-35] LR: 0.0010: iter loss: 0.511, iter iou: 0.119, epoch loss: 0.922, epoch iou: 0.117,  time cost: 9.753 s\n","[Epoch-0 Iter-36] LR: 0.0010: iter loss: 0.429, iter iou: 0.122, epoch loss: 0.909, epoch iou: 0.117,  time cost: 9.847 s\n","[Epoch-0 Iter-37] LR: 0.0010: iter loss: 0.458, iter iou: 0.120, epoch loss: 0.896, epoch iou: 0.117,  time cost: 10.392 s\n","[Epoch-0 Iter-38] LR: 0.0010: iter loss: 0.462, iter iou: 0.120, epoch loss: 0.885, epoch iou: 0.117,  time cost: 9.904 s\n","[Epoch-0 Iter-39] LR: 0.0010: iter loss: 0.430, iter iou: 0.121, epoch loss: 0.873, epoch iou: 0.117,  time cost: 9.748 s\n","[Epoch-0 Iter-40] LR: 0.0010: iter loss: 0.415, iter iou: 0.121, epoch loss: 0.862, epoch iou: 0.117,  time cost: 9.581 s\n","[Epoch-0 Iter-41] LR: 0.0010: iter loss: 0.378, iter iou: 0.122, epoch loss: 0.850, epoch iou: 0.117,  time cost: 9.552 s\n","[Epoch-0 Iter-42] LR: 0.0010: iter loss: 0.359, iter iou: 0.122, epoch loss: 0.838, epoch iou: 0.117,  time cost: 10.076 s\n","[Epoch-0 Iter-43] LR: 0.0010: iter loss: 0.370, iter iou: 0.121, epoch loss: 0.827, epoch iou: 0.117,  time cost: 9.354 s\n","[Epoch-0 Iter-44] LR: 0.0010: iter loss: 0.496, iter iou: 0.116, epoch loss: 0.820, epoch iou: 0.117,  time cost: 9.994 s\n","[Epoch-0 Iter-45] LR: 0.0010: iter loss: 0.366, iter iou: 0.121, epoch loss: 0.810, epoch iou: 0.118,  time cost: 10.025 s\n","[Epoch-0 Iter-46] LR: 0.0010: iter loss: 0.307, iter iou: 0.123, epoch loss: 0.799, epoch iou: 0.118,  time cost: 9.496 s\n","[Epoch-0 Iter-47] LR: 0.0010: iter loss: 0.337, iter iou: 0.121, epoch loss: 0.789, epoch iou: 0.118,  time cost: 10.208 s\n","[Epoch-0 Iter-48] LR: 0.0010: iter loss: 0.321, iter iou: 0.122, epoch loss: 0.779, epoch iou: 0.118,  time cost: 10.037 s\n","[Epoch-0 Iter-49] LR: 0.0010: iter loss: 0.367, iter iou: 0.120, epoch loss: 0.771, epoch iou: 0.118,  time cost: 10.741 s\n","[Epoch-0 Iter-50] LR: 0.0010: iter loss: 0.284, iter iou: 0.122, epoch loss: 0.761, epoch iou: 0.118,  time cost: 10.047 s\n","[Epoch-0 Iter-51] LR: 0.0010: iter loss: 0.284, iter iou: 0.122, epoch loss: 0.752, epoch iou: 0.118,  time cost: 10.271 s\n","[Epoch-0 Iter-52] LR: 0.0010: iter loss: 0.299, iter iou: 0.121, epoch loss: 0.743, epoch iou: 0.118,  time cost: 10.250 s\n","[Epoch-0 Iter-53] LR: 0.0010: iter loss: 0.404, iter iou: 0.117, epoch loss: 0.737, epoch iou: 0.118,  time cost: 9.768 s\n","[Epoch-0 Iter-54] LR: 0.0010: iter loss: 0.261, iter iou: 0.122, epoch loss: 0.728, epoch iou: 0.118,  time cost: 10.228 s\n","[Epoch-0 Iter-55] LR: 0.0010: iter loss: 0.254, iter iou: 0.122, epoch loss: 0.719, epoch iou: 0.118,  time cost: 10.140 s\n","[Epoch-0 Iter-56] LR: 0.0010: iter loss: 0.337, iter iou: 0.119, epoch loss: 0.712, epoch iou: 0.118,  time cost: 9.443 s\n","[Epoch-0 Iter-57] LR: 0.0010: iter loss: 0.231, iter iou: 0.123, epoch loss: 0.704, epoch iou: 0.118,  time cost: 9.653 s\n","[Epoch-0 Iter-58] LR: 0.0010: iter loss: 0.249, iter iou: 0.121, epoch loss: 0.696, epoch iou: 0.118,  time cost: 9.859 s\n","[Epoch-0 Iter-59] LR: 0.0010: iter loss: 0.258, iter iou: 0.121, epoch loss: 0.689, epoch iou: 0.118,  time cost: 9.366 s\n","[Epoch-0 Iter-60] LR: 0.0010: iter loss: 0.294, iter iou: 0.120, epoch loss: 0.682, epoch iou: 0.118,  time cost: 9.536 s\n","[Epoch-0 Iter-61] LR: 0.0010: iter loss: 0.285, iter iou: 0.120, epoch loss: 0.676, epoch iou: 0.118,  time cost: 9.736 s\n","[Epoch-0 Iter-62] LR: 0.0010: iter loss: 0.217, iter iou: 0.122, epoch loss: 0.668, epoch iou: 0.119,  time cost: 9.704 s\n","[Epoch-0 Iter-63] LR: 0.0010: iter loss: 0.227, iter iou: 0.122, epoch loss: 0.661, epoch iou: 0.119,  time cost: 10.711 s\n","[Epoch-0 Iter-64] LR: 0.0010: iter loss: 0.203, iter iou: 0.123, epoch loss: 0.654, epoch iou: 0.119,  time cost: 9.672 s\n","[Epoch-0 Iter-65] LR: 0.0010: iter loss: 0.329, iter iou: 0.118, epoch loss: 0.649, epoch iou: 0.119,  time cost: 9.854 s\n","[Epoch-0 Iter-66] LR: 0.0010: iter loss: 0.334, iter iou: 0.118, epoch loss: 0.644, epoch iou: 0.119,  time cost: 10.325 s\n","[Epoch-0 Iter-67] LR: 0.0010: iter loss: 0.237, iter iou: 0.121, epoch loss: 0.638, epoch iou: 0.119,  time cost: 9.938 s\n","[Epoch-0 Iter-68] LR: 0.0010: iter loss: 0.200, iter iou: 0.123, epoch loss: 0.632, epoch iou: 0.119,  time cost: 9.887 s\n","[Epoch-0 Iter-69] LR: 0.0010: iter loss: 0.221, iter iou: 0.121, epoch loss: 0.626, epoch iou: 0.119,  time cost: 10.320 s\n","[Epoch-0 Iter-70] LR: 0.0010: iter loss: 0.175, iter iou: 0.123, epoch loss: 0.619, epoch iou: 0.119,  time cost: 9.597 s\n","[Epoch-0 Iter-71] LR: 0.0010: iter loss: 0.173, iter iou: 0.123, epoch loss: 0.613, epoch iou: 0.119,  time cost: 9.924 s\n","[Epoch-0 Iter-72] LR: 0.0010: iter loss: 0.263, iter iou: 0.120, epoch loss: 0.608, epoch iou: 0.119,  time cost: 9.749 s\n","[Epoch-0 Iter-73] LR: 0.0010: iter loss: 0.329, iter iou: 0.118, epoch loss: 0.604, epoch iou: 0.119,  time cost: 10.054 s\n","[Epoch-0 Iter-74] LR: 0.0010: iter loss: 0.279, iter iou: 0.119, epoch loss: 0.600, epoch iou: 0.119,  time cost: 10.032 s\n","[Epoch-0 Iter-75] LR: 0.0010: iter loss: 0.197, iter iou: 0.122, epoch loss: 0.595, epoch iou: 0.119,  time cost: 10.154 s\n","[Epoch-0 Iter-76] LR: 0.0010: iter loss: 0.209, iter iou: 0.121, epoch loss: 0.590, epoch iou: 0.119,  time cost: 10.251 s\n","[Epoch-0 Iter-77] LR: 0.0010: iter loss: 0.238, iter iou: 0.121, epoch loss: 0.585, epoch iou: 0.119,  time cost: 10.293 s\n","[Epoch-0 Iter-78] LR: 0.0010: iter loss: 0.152, iter iou: 0.123, epoch loss: 0.579, epoch iou: 0.119,  time cost: 10.142 s\n","[Epoch-0 Iter-79] LR: 0.0010: iter loss: 0.189, iter iou: 0.121, epoch loss: 0.575, epoch iou: 0.119,  time cost: 9.913 s\n","[Epoch-0 Iter-80] LR: 0.0010: iter loss: 0.168, iter iou: 0.132, epoch loss: 0.569, epoch iou: 0.119,  time cost: 9.868 s\n","[Epoch-0 Iter-81] LR: 0.0010: iter loss: 0.154, iter iou: 0.123, epoch loss: 0.564, epoch iou: 0.119,  time cost: 9.517 s\n","[Epoch-0 Iter-82] LR: 0.0010: iter loss: 0.343, iter iou: 0.117, epoch loss: 0.562, epoch iou: 0.119,  time cost: 10.428 s\n","[Epoch-0 Iter-83] LR: 0.0010: iter loss: 0.165, iter iou: 0.122, epoch loss: 0.557, epoch iou: 0.119,  time cost: 10.071 s\n","[Epoch-0 Iter-84] LR: 0.0010: iter loss: 0.157, iter iou: 0.122, epoch loss: 0.552, epoch iou: 0.119,  time cost: 10.069 s\n","[Epoch-0 Iter-85] LR: 0.0010: iter loss: 0.147, iter iou: 0.123, epoch loss: 0.547, epoch iou: 0.119,  time cost: 10.024 s\n","[Epoch-0 Iter-86] LR: 0.0010: iter loss: 0.165, iter iou: 0.122, epoch loss: 0.543, epoch iou: 0.119,  time cost: 9.675 s\n","[Epoch-0 Iter-87] LR: 0.0010: iter loss: 0.205, iter iou: 0.121, epoch loss: 0.539, epoch iou: 0.119,  time cost: 9.788 s\n","[Epoch-0 Iter-88] LR: 0.0010: iter loss: 0.156, iter iou: 0.122, epoch loss: 0.535, epoch iou: 0.119,  time cost: 11.224 s\n","[Epoch-0 Iter-89] LR: 0.0010: iter loss: 0.164, iter iou: 0.121, epoch loss: 0.530, epoch iou: 0.119,  time cost: 10.210 s\n","[Epoch-0 Iter-90] LR: 0.0010: iter loss: 0.191, iter iou: 0.120, epoch loss: 0.527, epoch iou: 0.119,  time cost: 9.825 s\n","[Epoch-0 Iter-91] LR: 0.0010: iter loss: 0.143, iter iou: 0.122, epoch loss: 0.522, epoch iou: 0.119,  time cost: 9.808 s\n","[Epoch-0 Iter-92] LR: 0.0010: iter loss: 0.127, iter iou: 0.123, epoch loss: 0.518, epoch iou: 0.119,  time cost: 9.363 s\n","[Epoch-0 Iter-93] LR: 0.0010: iter loss: 0.166, iter iou: 0.121, epoch loss: 0.514, epoch iou: 0.119,  time cost: 10.184 s\n","[Epoch-0 Iter-94] LR: 0.0010: iter loss: 0.172, iter iou: 0.121, epoch loss: 0.511, epoch iou: 0.120,  time cost: 9.568 s\n","[Epoch-0 Iter-95] LR: 0.0010: iter loss: 0.232, iter iou: 0.120, epoch loss: 0.508, epoch iou: 0.120,  time cost: 9.884 s\n","[Epoch-0 Iter-96] LR: 0.0010: iter loss: 0.187, iter iou: 0.121, epoch loss: 0.504, epoch iou: 0.120,  time cost: 9.963 s\n","[Epoch-0 Iter-97] LR: 0.0010: iter loss: 0.214, iter iou: 0.120, epoch loss: 0.501, epoch iou: 0.120,  time cost: 9.378 s\n","[Epoch-0 Iter-98] LR: 0.0010: iter loss: 0.135, iter iou: 0.122, epoch loss: 0.498, epoch iou: 0.120,  time cost: 9.740 s\n","[Epoch-0 Iter-99] LR: 0.0010: iter loss: 0.113, iter iou: 0.123, epoch loss: 0.494, epoch iou: 0.120,  time cost: 9.755 s\n","[Epoch-0 Iter-100] LR: 0.0010: iter loss: 0.177, iter iou: 0.120, epoch loss: 0.491, epoch iou: 0.120,  time cost: 10.514 s\n","[Epoch-0 Iter-101] LR: 0.0010: iter loss: 0.151, iter iou: 0.122, epoch loss: 0.487, epoch iou: 0.120,  time cost: 10.270 s\n","[Epoch-0 Iter-102] LR: 0.0010: iter loss: 0.162, iter iou: 0.120, epoch loss: 0.484, epoch iou: 0.120,  time cost: 10.187 s\n","[Epoch-0 Iter-103] LR: 0.0010: iter loss: 0.138, iter iou: 0.159, epoch loss: 0.481, epoch iou: 0.120,  time cost: 9.797 s\n","[Epoch-0 Iter-104] LR: 0.0010: iter loss: 0.135, iter iou: 0.149, epoch loss: 0.477, epoch iou: 0.120,  time cost: 9.529 s\n","[Epoch-0 Iter-105] LR: 0.0010: iter loss: 0.177, iter iou: 0.139, epoch loss: 0.475, epoch iou: 0.120,  time cost: 9.708 s\n","[Epoch-0 Iter-106] LR: 0.0010: iter loss: 0.111, iter iou: 0.168, epoch loss: 0.471, epoch iou: 0.121,  time cost: 9.993 s\n","[Epoch-0 Iter-107] LR: 0.0010: iter loss: 0.125, iter iou: 0.167, epoch loss: 0.468, epoch iou: 0.121,  time cost: 9.623 s\n","[Epoch-0 Iter-108] LR: 0.0010: iter loss: 0.262, iter iou: 0.169, epoch loss: 0.466, epoch iou: 0.122,  time cost: 9.948 s\n","[Epoch-0 Iter-109] LR: 0.0010: iter loss: 0.143, iter iou: 0.161, epoch loss: 0.463, epoch iou: 0.122,  time cost: 10.031 s\n","[Epoch-0 Iter-110] LR: 0.0010: iter loss: 0.148, iter iou: 0.187, epoch loss: 0.460, epoch iou: 0.123,  time cost: 10.354 s\n","[Epoch-0 Iter-111] LR: 0.0010: iter loss: 0.099, iter iou: 0.183, epoch loss: 0.457, epoch iou: 0.123,  time cost: 9.727 s\n","[Epoch-0 Iter-112] LR: 0.0010: iter loss: 0.113, iter iou: 0.175, epoch loss: 0.454, epoch iou: 0.124,  time cost: 9.640 s\n","[Epoch-0 Iter-113] LR: 0.0010: iter loss: 0.113, iter iou: 0.177, epoch loss: 0.451, epoch iou: 0.124,  time cost: 9.633 s\n","[Epoch-0 Iter-114] LR: 0.0010: iter loss: 0.122, iter iou: 0.164, epoch loss: 0.448, epoch iou: 0.125,  time cost: 9.853 s\n","[Epoch-0 Iter-115] LR: 0.0010: iter loss: 0.211, iter iou: 0.161, epoch loss: 0.446, epoch iou: 0.125,  time cost: 9.832 s\n","[Epoch-0 Iter-116] LR: 0.0010: iter loss: 0.174, iter iou: 0.179, epoch loss: 0.444, epoch iou: 0.125,  time cost: 9.769 s\n","[Epoch-0 Iter-117] LR: 0.0010: iter loss: 0.208, iter iou: 0.156, epoch loss: 0.442, epoch iou: 0.126,  time cost: 9.741 s\n","[Epoch-0 Iter-118] LR: 0.0010: iter loss: 0.125, iter iou: 0.164, epoch loss: 0.439, epoch iou: 0.126,  time cost: 9.753 s\n","[Epoch-0 Iter-119] LR: 0.0010: iter loss: 0.094, iter iou: 0.168, epoch loss: 0.436, epoch iou: 0.126,  time cost: 10.103 s\n","[Epoch-0 Iter-120] LR: 0.0010: iter loss: 0.094, iter iou: 0.191, epoch loss: 0.433, epoch iou: 0.127,  time cost: 10.145 s\n","[Epoch-0 Iter-121] LR: 0.0010: iter loss: 0.094, iter iou: 0.175, epoch loss: 0.430, epoch iou: 0.127,  time cost: 9.788 s\n","[Epoch-0 Iter-122] LR: 0.0010: iter loss: 0.234, iter iou: 0.153, epoch loss: 0.429, epoch iou: 0.127,  time cost: 9.735 s\n","[Epoch-0 Iter-123] LR: 0.0010: iter loss: 0.358, iter iou: 0.144, epoch loss: 0.428, epoch iou: 0.128,  time cost: 9.302 s\n","[Epoch-0 Iter-124] LR: 0.0010: iter loss: 0.109, iter iou: 0.182, epoch loss: 0.426, epoch iou: 0.128,  time cost: 9.659 s\n","[Epoch-0 Iter-125] LR: 0.0010: iter loss: 0.089, iter iou: 0.179, epoch loss: 0.423, epoch iou: 0.128,  time cost: 9.864 s\n","[Epoch-0 Iter-126] LR: 0.0010: iter loss: 0.108, iter iou: 0.179, epoch loss: 0.420, epoch iou: 0.129,  time cost: 10.572 s\n","[Epoch-0 Iter-127] LR: 0.0010: iter loss: 0.202, iter iou: 0.148, epoch loss: 0.419, epoch iou: 0.129,  time cost: 10.437 s\n","[Epoch-0 Iter-128] LR: 0.0010: iter loss: 0.096, iter iou: 0.190, epoch loss: 0.416, epoch iou: 0.129,  time cost: 10.318 s\n","[Epoch-0 Iter-129] LR: 0.0010: iter loss: 0.092, iter iou: 0.175, epoch loss: 0.414, epoch iou: 0.130,  time cost: 10.664 s\n","[Epoch-0 Iter-130] LR: 0.0010: iter loss: 0.106, iter iou: 0.179, epoch loss: 0.411, epoch iou: 0.130,  time cost: 9.475 s\n","[Epoch-0 Iter-131] LR: 0.0010: iter loss: 0.183, iter iou: 0.134, epoch loss: 0.409, epoch iou: 0.130,  time cost: 9.749 s\n","[Epoch-0 Iter-132] LR: 0.0010: iter loss: 0.195, iter iou: 0.133, epoch loss: 0.408, epoch iou: 0.130,  time cost: 9.276 s\n","[Epoch-0 Iter-133] LR: 0.0010: iter loss: 0.188, iter iou: 0.145, epoch loss: 0.406, epoch iou: 0.130,  time cost: 9.345 s\n","[Epoch-0 Iter-134] LR: 0.0010: iter loss: 0.164, iter iou: 0.154, epoch loss: 0.404, epoch iou: 0.131,  time cost: 9.363 s\n","[Epoch-0 Iter-135] LR: 0.0010: iter loss: 0.198, iter iou: 0.175, epoch loss: 0.403, epoch iou: 0.131,  time cost: 10.166 s\n","[Epoch-0 Iter-136] LR: 0.0010: iter loss: 0.113, iter iou: 0.171, epoch loss: 0.401, epoch iou: 0.131,  time cost: 9.593 s\n","[Epoch-0 Iter-137] LR: 0.0010: iter loss: 0.151, iter iou: 0.173, epoch loss: 0.399, epoch iou: 0.131,  time cost: 9.824 s\n","[Epoch-0 Iter-138] LR: 0.0010: iter loss: 0.097, iter iou: 0.187, epoch loss: 0.397, epoch iou: 0.132,  time cost: 9.751 s\n","[Epoch-0 Iter-139] LR: 0.0010: iter loss: 0.095, iter iou: 0.187, epoch loss: 0.395, epoch iou: 0.132,  time cost: 9.718 s\n","[Epoch-0 Iter-140] LR: 0.0010: iter loss: 0.188, iter iou: 0.153, epoch loss: 0.393, epoch iou: 0.132,  time cost: 9.176 s\n","[Epoch-0 Iter-141] LR: 0.0010: iter loss: 0.123, iter iou: 0.144, epoch loss: 0.391, epoch iou: 0.132,  time cost: 9.765 s\n","[Epoch-0 Iter-142] LR: 0.0010: iter loss: 0.089, iter iou: 0.172, epoch loss: 0.389, epoch iou: 0.133,  time cost: 9.565 s\n","[Epoch-0 Iter-143] LR: 0.0010: iter loss: 0.127, iter iou: 0.153, epoch loss: 0.387, epoch iou: 0.133,  time cost: 9.914 s\n","[Epoch-0 Iter-144] LR: 0.0010: iter loss: 0.133, iter iou: 0.151, epoch loss: 0.385, epoch iou: 0.133,  time cost: 9.533 s\n","[Epoch-0 Iter-145] LR: 0.0010: iter loss: 0.101, iter iou: 0.150, epoch loss: 0.383, epoch iou: 0.133,  time cost: 9.949 s\n","[Epoch-0 Iter-146] LR: 0.0010: iter loss: 0.199, iter iou: 0.135, epoch loss: 0.382, epoch iou: 0.133,  time cost: 9.833 s\n","[Epoch-0 Iter-147] LR: 0.0010: iter loss: 0.117, iter iou: 0.157, epoch loss: 0.380, epoch iou: 0.133,  time cost: 9.776 s\n","[Epoch-0 Iter-148] LR: 0.0010: iter loss: 0.077, iter iou: 0.168, epoch loss: 0.378, epoch iou: 0.134,  time cost: 9.613 s\n","[Epoch-0 Iter-149] LR: 0.0010: iter loss: 0.241, iter iou: 0.134, epoch loss: 0.377, epoch iou: 0.134,  time cost: 10.090 s\n","[Epoch-0 Iter-150] LR: 0.0010: iter loss: 0.097, iter iou: 0.165, epoch loss: 0.376, epoch iou: 0.134,  time cost: 10.233 s\n","[Epoch-0 Iter-151] LR: 0.0010: iter loss: 0.088, iter iou: 0.178, epoch loss: 0.374, epoch iou: 0.134,  time cost: 10.606 s\n","[Epoch-0 Iter-152] LR: 0.0010: iter loss: 0.165, iter iou: 0.166, epoch loss: 0.372, epoch iou: 0.134,  time cost: 9.785 s\n","[Epoch-0 Iter-153] LR: 0.0010: iter loss: 0.102, iter iou: 0.159, epoch loss: 0.371, epoch iou: 0.134,  time cost: 9.620 s\n","[Epoch-0 Iter-154] LR: 0.0010: iter loss: 0.098, iter iou: 0.189, epoch loss: 0.369, epoch iou: 0.135,  time cost: 10.116 s\n","[Epoch-0 Iter-155] LR: 0.0010: iter loss: 0.304, iter iou: 0.167, epoch loss: 0.368, epoch iou: 0.135,  time cost: 9.617 s\n","[Epoch-0 Iter-156] LR: 0.0010: iter loss: 0.122, iter iou: 0.175, epoch loss: 0.367, epoch iou: 0.135,  time cost: 9.775 s\n","[Epoch-0 Iter-157] LR: 0.0010: iter loss: 0.173, iter iou: 0.154, epoch loss: 0.366, epoch iou: 0.135,  time cost: 10.295 s\n","[Epoch-0 Iter-158] LR: 0.0010: iter loss: 0.091, iter iou: 0.170, epoch loss: 0.364, epoch iou: 0.136,  time cost: 9.625 s\n","[Epoch-0 Iter-159] LR: 0.0010: iter loss: 0.186, iter iou: 0.154, epoch loss: 0.363, epoch iou: 0.136,  time cost: 9.268 s\n","[Epoch-0 Iter-160] LR: 0.0010: iter loss: 0.100, iter iou: 0.183, epoch loss: 0.361, epoch iou: 0.136,  time cost: 10.154 s\n","[Epoch-0 Iter-161] LR: 0.0010: iter loss: 0.133, iter iou: 0.157, epoch loss: 0.360, epoch iou: 0.136,  time cost: 9.711 s\n","[Epoch-0 Iter-162] LR: 0.0010: iter loss: 0.200, iter iou: 0.159, epoch loss: 0.359, epoch iou: 0.136,  time cost: 9.299 s\n","[Epoch-0 Iter-163] LR: 0.0010: iter loss: 0.091, iter iou: 0.174, epoch loss: 0.357, epoch iou: 0.137,  time cost: 9.602 s\n","[Epoch-0 Iter-164] LR: 0.0010: iter loss: 0.106, iter iou: 0.158, epoch loss: 0.355, epoch iou: 0.137,  time cost: 9.475 s\n","[Epoch-0 Iter-165] LR: 0.0010: iter loss: 0.130, iter iou: 0.168, epoch loss: 0.354, epoch iou: 0.137,  time cost: 9.585 s\n","[Epoch-0 Iter-166] LR: 0.0010: iter loss: 0.156, iter iou: 0.163, epoch loss: 0.353, epoch iou: 0.137,  time cost: 10.589 s\n","[Epoch-0 Iter-167] LR: 0.0010: iter loss: 0.073, iter iou: 0.169, epoch loss: 0.351, epoch iou: 0.137,  time cost: 9.836 s\n","[Epoch-0 Iter-168] LR: 0.0010: iter loss: 0.073, iter iou: 0.180, epoch loss: 0.350, epoch iou: 0.137,  time cost: 9.592 s\n","[Epoch-0 Iter-169] LR: 0.0010: iter loss: 0.076, iter iou: 0.171, epoch loss: 0.348, epoch iou: 0.138,  time cost: 10.097 s\n","[Epoch-0 Iter-170] LR: 0.0010: iter loss: 0.282, iter iou: 0.126, epoch loss: 0.348, epoch iou: 0.138,  time cost: 10.026 s\n","[Epoch-0 Iter-171] LR: 0.0010: iter loss: 0.095, iter iou: 0.169, epoch loss: 0.346, epoch iou: 0.138,  time cost: 9.697 s\n","[Epoch-0 Iter-172] LR: 0.0010: iter loss: 0.135, iter iou: 0.163, epoch loss: 0.345, epoch iou: 0.138,  time cost: 10.227 s\n","[Epoch-0 Iter-173] LR: 0.0010: iter loss: 0.136, iter iou: 0.183, epoch loss: 0.344, epoch iou: 0.138,  time cost: 10.091 s\n","[Epoch-0 Iter-174] LR: 0.0010: iter loss: 0.103, iter iou: 0.172, epoch loss: 0.342, epoch iou: 0.138,  time cost: 9.869 s\n","[Epoch-0 Iter-175] LR: 0.0010: iter loss: 0.128, iter iou: 0.159, epoch loss: 0.341, epoch iou: 0.138,  time cost: 9.786 s\n","[Epoch-0 Iter-176] LR: 0.0010: iter loss: 0.103, iter iou: 0.165, epoch loss: 0.340, epoch iou: 0.139,  time cost: 9.483 s\n","[Epoch-0 Iter-177] LR: 0.0010: iter loss: 0.157, iter iou: 0.180, epoch loss: 0.339, epoch iou: 0.139,  time cost: 10.175 s\n","[Epoch-0 Iter-178] LR: 0.0010: iter loss: 0.214, iter iou: 0.148, epoch loss: 0.338, epoch iou: 0.139,  time cost: 9.969 s\n","[Epoch-0 Iter-179] LR: 0.0010: iter loss: 0.093, iter iou: 0.166, epoch loss: 0.337, epoch iou: 0.139,  time cost: 10.405 s\n","[Epoch-0 Iter-180] LR: 0.0010: iter loss: 0.133, iter iou: 0.189, epoch loss: 0.335, epoch iou: 0.139,  time cost: 9.124 s\n","[Epoch-0 Iter-181] LR: 0.0010: iter loss: 0.181, iter iou: 0.149, epoch loss: 0.335, epoch iou: 0.139,  time cost: 9.643 s\n","[Epoch-0 Iter-182] LR: 0.0010: iter loss: 0.262, iter iou: 0.140, epoch loss: 0.334, epoch iou: 0.139,  time cost: 10.106 s\n","[Epoch-0 Iter-183] LR: 0.0010: iter loss: 0.130, iter iou: 0.162, epoch loss: 0.333, epoch iou: 0.140,  time cost: 9.873 s\n","[Epoch-0 Iter-184] LR: 0.0010: iter loss: 0.089, iter iou: 0.175, epoch loss: 0.332, epoch iou: 0.140,  time cost: 9.834 s\n","[Epoch-0 Iter-185] LR: 0.0010: iter loss: 0.091, iter iou: 0.182, epoch loss: 0.330, epoch iou: 0.140,  time cost: 10.198 s\n","[Epoch-0 Iter-186] LR: 0.0010: iter loss: 0.162, iter iou: 0.149, epoch loss: 0.330, epoch iou: 0.140,  time cost: 9.643 s\n","[Epoch-0 Iter-187] LR: 0.0010: iter loss: 0.064, iter iou: 0.177, epoch loss: 0.328, epoch iou: 0.140,  time cost: 9.732 s\n","[Epoch-0 Iter-188] LR: 0.0010: iter loss: 0.074, iter iou: 0.177, epoch loss: 0.327, epoch iou: 0.140,  time cost: 9.984 s\n","[Epoch-0 Iter-189] LR: 0.0010: iter loss: 0.101, iter iou: 0.163, epoch loss: 0.326, epoch iou: 0.140,  time cost: 9.666 s\n","[Epoch-0 Iter-190] LR: 0.0010: iter loss: 0.096, iter iou: 0.161, epoch loss: 0.324, epoch iou: 0.141,  time cost: 10.000 s\n","[Epoch-0 Iter-191] LR: 0.0010: iter loss: 0.143, iter iou: 0.181, epoch loss: 0.323, epoch iou: 0.141,  time cost: 10.856 s\n","[Epoch-0 Iter-192] LR: 0.0010: iter loss: 0.123, iter iou: 0.169, epoch loss: 0.322, epoch iou: 0.141,  time cost: 9.365 s\n","[Epoch-0 Iter-193] LR: 0.0010: iter loss: 0.096, iter iou: 0.181, epoch loss: 0.321, epoch iou: 0.141,  time cost: 9.650 s\n","[Epoch-0 Iter-194] LR: 0.0010: iter loss: 0.132, iter iou: 0.160, epoch loss: 0.320, epoch iou: 0.141,  time cost: 9.867 s\n","[Epoch-0 Iter-195] LR: 0.0010: iter loss: 0.075, iter iou: 0.174, epoch loss: 0.319, epoch iou: 0.141,  time cost: 10.116 s\n","[Epoch-0 Iter-196] LR: 0.0010: iter loss: 0.174, iter iou: 0.144, epoch loss: 0.318, epoch iou: 0.141,  time cost: 9.990 s\n","[Epoch-0 Iter-197] LR: 0.0010: iter loss: 0.144, iter iou: 0.163, epoch loss: 0.317, epoch iou: 0.142,  time cost: 9.769 s\n","[Epoch-0 Iter-198] LR: 0.0010: iter loss: 0.078, iter iou: 0.194, epoch loss: 0.316, epoch iou: 0.142,  time cost: 10.099 s\n","[Epoch-0 Iter-199] LR: 0.0010: iter loss: 0.155, iter iou: 0.152, epoch loss: 0.315, epoch iou: 0.142,  time cost: 9.910 s\n","[Epoch-0 Iter-200] LR: 0.0010: iter loss: 0.105, iter iou: 0.174, epoch loss: 0.314, epoch iou: 0.142,  time cost: 9.878 s\n","[Epoch-0 Iter-201] LR: 0.0010: iter loss: 0.085, iter iou: 0.188, epoch loss: 0.313, epoch iou: 0.142,  time cost: 10.065 s\n","[Epoch-0 Iter-202] LR: 0.0010: iter loss: 0.122, iter iou: 0.170, epoch loss: 0.312, epoch iou: 0.142,  time cost: 9.890 s\n","[Epoch-0 Iter-203] LR: 0.0010: iter loss: 0.089, iter iou: 0.172, epoch loss: 0.311, epoch iou: 0.143,  time cost: 10.374 s\n","[Epoch-0 Iter-204] LR: 0.0010: iter loss: 0.114, iter iou: 0.172, epoch loss: 0.310, epoch iou: 0.143,  time cost: 9.888 s\n","[Epoch-0 Iter-205] LR: 0.0010: iter loss: 0.242, iter iou: 0.166, epoch loss: 0.310, epoch iou: 0.143,  time cost: 9.601 s\n","[Epoch-0 Iter-206] LR: 0.0010: iter loss: 0.105, iter iou: 0.173, epoch loss: 0.309, epoch iou: 0.143,  time cost: 9.569 s\n","[Epoch-0 Iter-207] LR: 0.0010: iter loss: 0.181, iter iou: 0.145, epoch loss: 0.308, epoch iou: 0.143,  time cost: 9.660 s\n","[Epoch-0 Iter-208] LR: 0.0010: iter loss: 0.236, iter iou: 0.146, epoch loss: 0.308, epoch iou: 0.143,  time cost: 10.224 s\n","[Epoch-0 Iter-209] LR: 0.0010: iter loss: 0.128, iter iou: 0.174, epoch loss: 0.307, epoch iou: 0.143,  time cost: 9.764 s\n","[Epoch-0 Iter-210] LR: 0.0010: iter loss: 0.166, iter iou: 0.151, epoch loss: 0.306, epoch iou: 0.143,  time cost: 9.395 s\n","[Epoch-0 Iter-211] LR: 0.0010: iter loss: 0.101, iter iou: 0.188, epoch loss: 0.305, epoch iou: 0.143,  time cost: 10.289 s\n","[Epoch-0 Iter-212] LR: 0.0010: iter loss: 0.168, iter iou: 0.170, epoch loss: 0.305, epoch iou: 0.143,  time cost: 10.207 s\n","[Epoch-0 Iter-213] LR: 0.0010: iter loss: 0.124, iter iou: 0.158, epoch loss: 0.304, epoch iou: 0.144,  time cost: 9.949 s\n","[Epoch-0 Iter-214] LR: 0.0010: iter loss: 0.247, iter iou: 0.163, epoch loss: 0.304, epoch iou: 0.144,  time cost: 9.377 s\n","[Epoch-0 Iter-215] LR: 0.0010: iter loss: 0.103, iter iou: 0.192, epoch loss: 0.303, epoch iou: 0.144,  time cost: 10.084 s\n","[Epoch-0 Iter-216] LR: 0.0010: iter loss: 0.172, iter iou: 0.155, epoch loss: 0.302, epoch iou: 0.144,  time cost: 9.570 s\n","[Epoch-0 Iter-217] LR: 0.0010: iter loss: 0.083, iter iou: 0.172, epoch loss: 0.301, epoch iou: 0.144,  time cost: 9.688 s\n","[Epoch-0 Iter-218] LR: 0.0010: iter loss: 0.206, iter iou: 0.141, epoch loss: 0.301, epoch iou: 0.144,  time cost: 9.554 s\n","[Epoch-0 Iter-219] LR: 0.0010: iter loss: 0.066, iter iou: 0.181, epoch loss: 0.300, epoch iou: 0.144,  time cost: 9.579 s\n","[Epoch-0 Iter-220] LR: 0.0010: iter loss: 0.121, iter iou: 0.172, epoch loss: 0.299, epoch iou: 0.144,  time cost: 9.596 s\n","[Epoch-0 Iter-221] LR: 0.0010: iter loss: 0.092, iter iou: 0.175, epoch loss: 0.298, epoch iou: 0.144,  time cost: 9.565 s\n","[Epoch-0 Iter-222] LR: 0.0010: iter loss: 0.282, iter iou: 0.171, epoch loss: 0.298, epoch iou: 0.145,  time cost: 9.646 s\n","[Epoch-0 Iter-223] LR: 0.0010: iter loss: 0.093, iter iou: 0.166, epoch loss: 0.297, epoch iou: 0.145,  time cost: 10.472 s\n","[Epoch-0 Iter-224] LR: 0.0010: iter loss: 0.153, iter iou: 0.169, epoch loss: 0.296, epoch iou: 0.145,  time cost: 9.163 s\n","[Epoch-0 Iter-225] LR: 0.0010: iter loss: 0.088, iter iou: 0.185, epoch loss: 0.295, epoch iou: 0.145,  time cost: 10.079 s\n","[Epoch-0 Iter-226] LR: 0.0010: iter loss: 0.106, iter iou: 0.180, epoch loss: 0.294, epoch iou: 0.145,  time cost: 9.515 s\n"]}]},{"cell_type":"code","source":["torch.save(net.state_dict(), \"result.pt\")"],"metadata":{"id":"GYtY0V5qa9bF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(net.state_dict(), \n","                    pjoin(cfg.WEIGHTS_SAVE_ROOT, \"weights_ep_%d_%.3f_%.3f.pth\" \n","                            % (epoch, epoch_loss / epoch_size, epoch_miou / epoch_size)))\n","    "],"metadata":{"id":"gTPTFdlJa7pp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.figure(figsize=(10, 5))\n","plt.suptitle('dice loss + cross-entropy training')\n","plt.subplot(1, 2, 1)\n","plt.plot(loss_plot)\n","plt.ylabel('loss')\n","plt.xlabel('iteration')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(iou_plot)\n","plt.ylabel('Image-IoU (%)')\n","plt.xlabel('iteration')"],"metadata":{"id":"5jFi6Ed1eAJC"},"execution_count":null,"outputs":[]}]}