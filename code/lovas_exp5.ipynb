{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23889,"status":"ok","timestamp":1650209139634,"user":{"displayName":"Kili","userId":"18406356377422180073"},"user_tz":240},"id":"hV3OfnRxglel","outputId":"182ffe21-e59a-444c-eb30-70b69e227d02"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sM6qOXiJK2hf"},"outputs":[],"source":["import sys\n","sys.path.append('/content/drive/Shareddrives/EECS 545 Project/5DE/lane-detection-2019-howard')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MDTNSK_bQCqL"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torch.nn.functional as F\n","class UNetFactory(nn.Module):\n","    \"\"\"\n","    本质上就是一个U型的网络，先encode，后decode，中间可能有架bridge。\n","    其中encoder需要输出skip到decode那边做concatenate，使得decode阶段能补充信息。\n","    bridge不能存在下采样和上采样的操作。\n","    \"\"\"\n","    def __init__(self, encoder_blocks, decoder_blocks, bridge=None):\n","        super(UNetFactory, self).__init__()\n","        self.encoder = UNetEncoder(encoder_blocks)\n","        self.bridge = bridge\n","        self.decoder = UNetDecoder(decoder_blocks)\n","\n","    def forward(self, x):\n","        res = self.encoder(x)\n","        out, skips = res[0], res[1:]\n","        if self.bridge is not None:\n","            out = self.bridge(out)\n","        out = self.decoder(out, skips)\n","        return out\n","\n","class UNetEncoder(nn.Module):\n","    \"\"\"\n","    encoder会有多次下采样，下采样前的feature map要作为skip缓存起来将来送到decoder用。\n","    这里约定，以下采样为界线，将encoder分成多个block，其中第一个block无下采样操作，后面的每个block内都\n","    含有一次下采样操作。\n","    \"\"\"\n","    def __init__(self, blocks):\n","        super(UNetEncoder, self).__init__()\n","        assert len(blocks) > 0\n","        self.blocks = nn.ModuleList(blocks)\n","\n","    def forward(self, x):\n","        skips = []\n","        for i in range(len(self.blocks) - 1):\n","            x = self.blocks[i](x)\n","            skips.append(x)\n","        res = [self.blocks[i+1](x)]\n","        res += skips\n","        return res # 只能以这种方式返回多个tensor\n","\n","class UNetDecoder(nn.Module):\n","    \"\"\"\n","    decoder会有多次上采样，每次上采样后，要跟相应的skip做concatenate。\n","    这里约定，以上采样为界线，将decoder分成多个block，其中最后一个block无上采样操作，其他block内\n","    都含有一次上采样。如此一来，除第一个block以外，其他block都先做concatenate。\n","    \"\"\"\n","    def __init__(self, blocks):\n","        super(UNetDecoder, self).__init__()\n","        assert len(blocks) > 1\n","        self.blocks = nn.ModuleList(blocks)\n","    \n","    def _center_crop(self, skip, x):\n","        \"\"\"\n","        skip和x，谁比较大，就裁剪谁\n","        \"\"\"\n","        _, _, h1, w1 = skip.shape\n","        _, _, h2, w2 = x.shape\n","        ht, wt = min(h1, h2), min(w1, w2)\n","        dh1 = (h1 - ht) // 2 if h1 > ht else 0\n","        dw1 = (w1 - wt) // 2 if w1 > wt else 0\n","        dh2 = (h2 - ht) // 2 if h2 > ht else 0\n","        dw2 = (w2 - wt) // 2 if w2 > wt else 0\n","        return skip[:, :, dh1: (dh1 + ht), dw1: (dw1 + wt)], \\\n","                x[:, :, dh2: (dh2 + ht), dw2: (dw2 + wt)]\n","\n","    def forward(self, x, skips, reverse_skips=True):\n","        assert len(skips) == len(self.blocks) - 1\n","        if reverse_skips:\n","            skips = skips[::-1]\n","        x = self.blocks[0](x)\n","        for i in range(1, len(self.blocks)):\n","            skip, x = self._center_crop(skips[i-1], x)\n","            x = torch.cat([skip, x], dim=1)\n","            x = self.blocks[i](x)\n","        return x\n","\n","def unet_convs(in_channels, out_channels, padding=0):\n","    \"\"\"\n","    unet论文里出现次数最多的2个conv3x3(non-padding)的结构\n","    \"\"\"\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=padding, bias=False),\n","        # nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=padding, bias=False),\n","        # nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True),\n","    )\n","\n","\n","def unet(in_channels, out_channels):\n","    \"\"\"\n","    构造跟论文一致的unet网络\n","    https://arxiv.org/abs/1505.04597\n","    \"\"\"\n","    # encoder\n","    encoder_blocks = [\n","        # two conv3x3\n","        unet_convs(in_channels, 64),\n","        # max pool 2x2, two conv3x3\n","        nn.Sequential(\n","            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n","            unet_convs(64, 128)\n","        ),\n","        # max pool 2x2, two conv3x3\n","        nn.Sequential(\n","            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n","            unet_convs(128, 256)\n","        ),\n","        # max pool 2x2, two conv3x3\n","        nn.Sequential(\n","            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n","            unet_convs(256, 512)\n","        ),\n","        # max pool 2x2\n","        nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n","    ]\n","    # bridge\n","    bridge = nn.Sequential(\n","        # two conv3x3\n","        unet_convs(512, 1024)\n","    )\n","    # decoder\n","    decoder_blocks = [\n","        # up-conv2x2\n","        nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2),\n","        # two conv3x3, up-conv2x2\n","        nn.Sequential(\n","            unet_convs(1024, 512),\n","            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n","        ),\n","        # two conv3x3, up-conv2x2\n","        nn.Sequential(\n","            unet_convs(512, 256),\n","            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n","        ),\n","        # two conv3x3, up-conv2x2\n","        nn.Sequential(\n","            unet_convs(256, 128),\n","            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n","        ),\n","        # two conv3x3, conv1x1\n","        nn.Sequential(\n","            unet_convs(128, 64),\n","            nn.Conv2d(64, out_channels, kernel_size=1)\n","        )\n","    ]\n","    return UNetFactory(encoder_blocks, decoder_blocks, bridge)\n","\n","def unet_resnet(resnet_type, in_channels, out_channels, pretrained=True):\n","    \"\"\"\n","    利用resnet作为encoder，相应地，decoder也做一些改动，使得输出的尺寸跟原始的一致\n","    \"\"\"\n","    if resnet_type == 'resnet18':\n","        resnet = torchvision.models.resnet.resnet18(pretrained)\n","        encoder_out_channels = [in_channels, 64, 64, 128, 256, 512]  # encoder各个block的输出channel\n","    elif resnet_type == 'resnet34':\n","        resnet = torchvision.models.resnet.resnet34(pretrained)\n","        encoder_out_channels = [in_channels, 64, 64, 128, 256, 512]\n","    elif resnet_type == 'resnet50':\n","        resnet = torchvision.models.resnet.resnet50(pretrained)\n","        encoder_out_channels = [in_channels, 64, 256, 512, 1024, 2048]\n","    elif resnet_type == 'resnet101':\n","        resnet = torchvision.models.resnet.resnet101(pretrained)\n","        encoder_out_channels = [in_channels, 64, 256, 512, 1024, 2048]\n","    elif resnet_type == 'resnet152':\n","        resnet = torchvision.models.resnet.resnet152(pretrained)\n","        encoder_out_channels = [in_channels, 64, 256, 512, 1024, 2048]\n","    elif resnet_type == 'resnext50_32x4d':\n","        resnet = torchvision.models.resnet.resnext50_32x4d(pretrained)\n","        encoder_out_channels = [in_channels, 64, 256, 512, 1024, 2048]\n","    else:\n","        raise ValueError(\"unexpected resnet_type\")\n","\n","    # encoder\n","    encoder_blocks = [\n","        # org input\n","        nn.Sequential(),\n","        # conv1\n","        nn.Sequential(\n","            resnet.conv1,\n","            resnet.bn1,\n","            resnet.relu\n","        ),\n","        # conv2_x\n","        nn.Sequential(\n","            resnet.maxpool,\n","            resnet.layer1\n","        ),\n","        # conv3_x\n","        resnet.layer2,\n","        # conv4_x\n","        resnet.layer3,\n","        # conv5_x\n","        resnet.layer4\n","    ]\n","    # bridge\n","    bridge = None  # 感觉并无必要\n","    # decoder\n","    decoder_blocks = []\n","    in_ch = encoder_out_channels[-1]\n","    out_ch = in_ch // 2\n","    decoder_blocks.append(nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)) # up-conv2x2\n","    for i in range(1, len(encoder_blocks)-1):\n","        in_ch = encoder_out_channels[-i-1] + out_ch  # cat\n","        decoder_blocks.append(nn.Sequential(  # two conv3x3, up-conv2x2\n","            unet_convs(in_ch, out_ch, padding=1),\n","            nn.ConvTranspose2d(out_ch, out_ch//2, kernel_size=2, stride=2),\n","        ))\n","        out_ch = out_ch // 2\n","    in_ch = encoder_out_channels[0] + out_ch  # cat\n","    decoder_blocks.append(nn.Sequential(  # two conv3x3, conv1x1\n","        unet_convs(in_ch, out_ch, padding=1),\n","        nn.Conv2d(out_ch, out_channels, kernel_size=1)\n","    ))\n","\n","    return UNetFactory(encoder_blocks, decoder_blocks, bridge)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rUbpIzPCRmNE"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":157},"executionInfo":{"elapsed":275,"status":"ok","timestamp":1650209147505,"user":{"displayName":"Kili","userId":"18406356377422180073"},"user_tz":240},"id":"dha6U_LzGxtJ","outputId":"5b5ca74f-7eb5-4871-dd16-ebc070ce77c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using the GPU. You are good to go!\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"    \\nclass ConfigInference(object):\\n    # 目录\\n    PROJECT_ROOT = dirname(abspath(__file__)) \\n    DATA_ROOT = pjoin(PROJECT_ROOT, 'data')\\n    IMAGE_ROOT = pjoin(DATA_ROOT, 'TestImage')\\n    LABEL_ROOT = pjoin(DATA_ROOT, 'TestLabel')\\n    OVERLAY_ROOT = pjoin(DATA_ROOT, 'TestOverlay')\\n    WEIGHTS_ROOT = pjoin(PROJECT_ROOT, 'weights')\\n    PRETRAINED_WEIGHTS = pjoin(WEIGHTS_ROOT, '1024x384_b4_unet_resnext50_32x4d', 'resnext50_32x4d-7cdf4587.pth')\\n    LOG_ROOT = pjoin(PROJECT_ROOT, 'logs')\\n\\n    # 设备\\n    DEVICE = 'cuda:0'\\n\\n    # 网络类型\\n    NET_NAME = 'resnext50_32x4d'\\n\\n    # 网络参数\\n    NUM_CLASSES = 8  # 8个类别\\n    # IMAGE_SIZE = (768, 256)  # 训练的图片的尺寸(h,w)\\n    IMAGE_SIZE = (1024, 384)  # 训练的图片的尺寸(h,w)\\n    # IMAGE_SIZE = (1536, 512)  # 训练的图片的尺寸(h,w)\\n    HEIGHT_CROP_OFFSET = 690  # 在height方向上将原图裁掉的offset\\n    BATCH_SIZE = 1  # 数据批次大小\\n\\n    # 原图的大小\\n    IMAGE_SIZE_ORG = (3384, 1710)\\n\""]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","@description: Configure Class \n","\"\"\"\n","\n","from os.path import join as pjoin\n","from os.path import dirname, abspath\n","import torch\n","\n","class ConfigTrain(object):\n","    # 目录\n","    PROJECT_ROOT = \"/content/drive/Shareddrives/EECS 545 Project/5DE/lane-detection-2019-howard\"\n","    DATA_LIST_ROOT = pjoin(PROJECT_ROOT, 'data_list')\n","    TRAIN_ROOT = \"/content/drive/Shareddrives/EECS 545 Project/data\"\n","    IMAGE_ROOT = pjoin(TRAIN_ROOT, 'Image_Data')\n","    LABEL_ROOT = pjoin(TRAIN_ROOT, 'Gray_Label')\n","    WEIGHTS_ROOT = pjoin(PROJECT_ROOT, 'weights')\n","    WEIGHTS_SAVE_ROOT = pjoin(WEIGHTS_ROOT, 'result')\n","    LOG_ROOT = pjoin(PROJECT_ROOT, 'logs')\n","\n","    # log文件\n","    # LOG_SUSPICIOUS_FILES = pjoin(LOG_ROOT, 'suspicious_files.log')\n","    # LOG_SUSPICIOUS_FILES = pjoin(LOG_ROOT, 'suspicious_files_b1.log')\n","    # LOG_SUSPICIOUS_FILES = pjoin(LOG_ROOT, 'suspicious_files_b4.log')\n","    LOG_SUSPICIOUS_FILES = pjoin(LOG_ROOT, 'suspicious_files_b2.log')\n","\n","    # 设备\n","    DEVICE = 'cuda:0' \n","    \n","    if torch.cuda.is_available():\n","      print(\"Using the GPU. You are good to go!\")\n","      DEVICE = 'cuda'\n","    else:\n","      print(\"Using the CPU. Overall speed may be slowed down\")\n","      DEVICE = 'cpu'\n","    \n","    # 网络类型\n","    # NET_NAME = 'unet_resnet101'\n","    NET_NAME = 'resnext50_32x4d'\n","    # 网络参数\n","    NUM_CLASSES = 8  # 8个类别\n","    IMAGE_SIZE = (768, 256)  # 训练的图片的尺寸(h,w)\n","    # IMAGE_SIZE = (1024, 384)  # 训练的图片的尺寸(h,w)\n","    #IMAGE_SIZE = (1536, 512)  # 训练的图片的尺寸(h,w)\n","    HEIGHT_CROP_OFFSET = 690  # 在height方向上将原图裁掉的offset\n","    # BATCH_SIZE = 8  # 数据批次大小\n","    # BATCH_SIZE = 1  # 数据批次大小\n","    # BATCH_SIZE = 4  # 数据批次大小\n","    BATCH_SIZE = 4  # 数据批次大小\n","    EPOCH_NUM = 8  # 总轮次\n","    PRETRAIN = False # 是否加载预训练的权重\n","    EPOCH_BEGIN = 0  # 接着前面的epoch训练，默认0，表示从头训练\n","    PRETRAINED_WEIGHTS = pjoin(WEIGHTS_ROOT, '1024x384_b4_unet_resnext50_32x4d', 'result_6.pt')\n","    BASE_LR = 0.001  # 学习率\n","    LR_STRATEGY = [\n","        [0.001], # epoch 0\n","        [0.001], # epoch 1\n","        [0.001], # epoch 2\n","        [0.001, 0.0006, 0.0003, 0.0001, 0.0004, 0.0008, 0.001], # epoch 3\n","        [0.001, 0.0006, 0.0003, 0.0001, 0.0004, 0.0008, 0.001], # epoch 4\n","        [0.001, 0.0006, 0.0003, 0.0001, 0.0004, 0.0008, 0.001], # epoch 5\n","        [0.0004, 0.0003, 0.0002, 0.0001, 0.0002, 0.0003, 0.0004], # epoch 6\n","        [0.0004, 0.0003, 0.0002, 0.0001, 0.0002, 0.0003, 0.0004], # epoch 7\n","    ]\n","    SUSPICIOUS_RATE = 0.8  # 可疑比例：当某个iteration的miou比当前epoch_miou的可疑比例还要小的时候，记录此次iteration的训练数据索引，人工排查是否数据有问题\n","\n","## TO DO: Define PROJECT_ROOT##\n","'''    \n","class ConfigInference(object):\n","    # 目录\n","    PROJECT_ROOT = dirname(abspath(__file__)) \n","    DATA_ROOT = pjoin(PROJECT_ROOT, 'data')\n","    IMAGE_ROOT = pjoin(DATA_ROOT, 'TestImage')\n","    LABEL_ROOT = pjoin(DATA_ROOT, 'TestLabel')\n","    OVERLAY_ROOT = pjoin(DATA_ROOT, 'TestOverlay')\n","    WEIGHTS_ROOT = pjoin(PROJECT_ROOT, 'weights')\n","    PRETRAINED_WEIGHTS = pjoin(WEIGHTS_ROOT, '1024x384_b4_unet_resnext50_32x4d', 'resnext50_32x4d-7cdf4587.pth')\n","    LOG_ROOT = pjoin(PROJECT_ROOT, 'logs')\n","\n","    # 设备\n","    DEVICE = 'cuda:0'\n","\n","    # 网络类型\n","    NET_NAME = 'resnext50_32x4d'\n","\n","    # 网络参数\n","    NUM_CLASSES = 8  # 8个类别\n","    # IMAGE_SIZE = (768, 256)  # 训练的图片的尺寸(h,w)\n","    IMAGE_SIZE = (1024, 384)  # 训练的图片的尺寸(h,w)\n","    # IMAGE_SIZE = (1536, 512)  # 训练的图片的尺寸(h,w)\n","    HEIGHT_CROP_OFFSET = 690  # 在height方向上将原图裁掉的offset\n","    BATCH_SIZE = 1  # 数据批次大小\n","\n","    # 原图的大小\n","    IMAGE_SIZE_ORG = (3384, 1710)\n","'''    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VhCWX0QNWYYI"},"outputs":[],"source":["'''\n","Define loss\n","'''\n","class MySoftmaxCrossEntropyLoss(nn.Module):\n","\n","    def __init__(self, nbclasses):\n","        super(MySoftmaxCrossEntropyLoss, self).__init__()\n","        self.nbclasses = nbclasses\n","\n","    def forward(self, inputs, target):\n","        if inputs.dim() > 2:\n","            inputs = inputs.view(inputs.size(0), inputs.size(1), -1)  # N,C,H,W => N,C,H*W\n","            inputs = inputs.transpose(1, 2)  # N,C,H*W => N,H*W,C\n","            inputs = inputs.contiguous().view(-1, self.nbclasses)  # N,H*W,C => N*H*W,C\n","        target = target.view(-1)\n","        return nn.CrossEntropyLoss(reduction=\"mean\")(inputs, target)\n","\n","\n","class FocalLoss(nn.Module):\n","\n","    def __init__(self, gamma=0, alpha=None, size_average=True):\n","        super(FocalLoss, self).__init__()\n","        self.gamma = gamma\n","        self.alpha = alpha\n","        self.alpha = torch.tensor([alpha, 1 - alpha])\n","        self.size_average = size_average\n","\n","    def forward(self, inputs, target):\n","        if inputs.dim() > 2:\n","            inputs = inputs\n","            inputs = inputs.view(inputs.size(0), inputs.size(1), -1)  # N,C,H,W => N,C,H*W\n","            inputs = inputs.transpose(1, 2)  # N,C,H*W => N,H*W,C\n","            inputs = inputs.contiguous().view(-1, inputs.size(2))  # N,H*W,C => N*H*W,C\n","        target = target.view(-1, 1)\n","\n","        logpt = F.log_softmax(inputs,dim=1)\n","        logpt = logpt.gather(1, target)\n","        logpt = logpt.view(-1)\n","        pt = logpt.exp()\n","\n","        if self.alpha is not None:\n","            if self.alpha.type() != inputs.data.type():\n","                self.alpha = self.alpha.type_as(inputs.data)\n","            at = self.alpha.gather(0, target.view(-1))\n","            logpt = logpt * at\n","        # mask = mask.view(-1)\n","        loss = -1 * (1 - pt) ** self.gamma * logpt #* mask\n","        if self.size_average:\n","            return loss.mean()\n","        else:\n","            return loss.sum()\n","\n","\n","def make_one_hot(input, num_classes):\n","    \"\"\"Convert class index tensor to one hot encoding tensor.\n","    Args:\n","         input: A tensor of shape [N, 1, *]\n","         num_classes: An int of number of class\n","    Returns:\n","        A tensor of shape [N, num_classes, *]\n","    \"\"\"\n","    shape = np.array(input.shape)\n","    shape[1] = num_classes\n","    shape = tuple(shape)\n","    result = torch.zeros(shape)\n","    result = result.scatter_(1, input.cpu(), 1)\n","\n","    return result\n","\n","\n","class BinaryDiceLoss(nn.Module):\n","    \"\"\"Dice loss of binary class\n","    Args:\n","        smooth: A float number to smooth loss, and avoid NaN error, default: 1\n","        p: Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2\n","        predict: A tensor of shape [N, *]\n","        target: A tensor of shape same with predict log文件\n","    # LOG_SUSPICIOUS_FILES = pjoin(LOG_ROOT, 'suspicious_files.log')\n","    # LOG_SUSPICIOUS_FILES = pjoin(LOG_ROOT, 'suspicious_files_b1.log')\n","    # LOG_SUSPICIOUS_FILES = pjoin(LOG_ROOT, 'suspicious_files_b4.log')\n","    LOG_SUSPICIOUS_FILES = pjoin(LOG_ROOT, 'suspicious_files_b2.log')\n","        reduction: Reduction method to apply, return mean over batch if 'mean',\n","            return sum if 'sum', return a tensor of shape [N,] if 'none'\n","    Returns:\n","        Loss tensor according to arg reduction\n","    Raise:\n","        Exception if unexpected reduction\n","    \"\"\"\n","    def __init__(self, smooth=1, p=2, reduction='mean'):\n","        super(BinaryDiceLoss, self).__init__()\n","        self.smooth = smooth\n","        self.p = p\n","        self.reduction = reduction\n","\n","    def forward(self, predict, target):\n","        assert predict.shape[0] == target.shape[0], \"predict & target batch size don't match\"\n","        predict = predict.contiguous().view(predict.shape[0], -1)\n","        target = target.contiguous().view(target.shape[0], -1)\n","        num = 2*torch.sum(torch.mul(predict, target), dim=1) + self.smooth\n","        den = torch.sum(predict.pow(self.p) + target.pow(self.p), dim=1) + self.smooth\n","\n","        loss = 1 - num / den\n","\n","        if self.reduction == 'mean':\n","            return loss.mean()\n","        elif self.reduction == 'sum':\n","            return loss.sum()\n","        elif self.reduction == 'none':\n","            return loss\n","        else:\n","            raise Exception('Unexpected reduction {}'.format(self.reduction))\n","\n","\n","class DiceLoss(nn.Module):\n","    \"\"\"Dice loss, need one hot encode input\n","    Args:\n","        weight: An array of shape [num_classes,]\n","        ignore_index: class index to ignore\n","        predict: A tensor of shape [N, C, *]\n","        target: A tensor of same shape with predict\n","        other args pass to BinaryDiceLoss\n","    Return:\n","        same as BinaryDiceLoss\n","    \"\"\"\n","    def __init__(self, weight=None, ignore_index=None, **kwargs):\n","        super(DiceLoss, self).__init__()\n","        self.kwargs = kwargs\n","        self.weight = weight\n","        self.ignore_index = ignore_index\n","\n","    def forward(self, predict, target):\n","        assert predict.shape == target.shape, 'predict & target shape do not match'\n","        dice = BinaryDiceLoss(**self.kwargs)\n","        total_loss = 0\n","        predict = F.softmax(predict, dim=1)\n","\n","        for i in range(target.shape[1]):\n","            if i != self.ignore_index:\n","                dice_loss = dice(predict[:, i], target[:, i])\n","                if self.weight is not None:\n","                    assert self.weight.shape[0] == target.shape[1], \\\n","                        'Expect weight shape [{}], get[{}]'.format(target.shape[1], self.weight.shape[0])\n","                    dice_loss *= self.weights[i]\n","                total_loss += dice_loss\n","\n","        return total_loss/target.shape[1]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EAFofjy4ekge"},"outputs":[],"source":["\n","\"\"\"\n","Lovasz-Softmax and Jaccard hinge loss in PyTorch\n","Maxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n","\"\"\"\n","\n","from __future__ import print_function, division\n","\n","import torch\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","import numpy as np\n","try:\n","    from itertools import  ifilterfalse\n","except ImportError: # py3k\n","    from itertools import  filterfalse as ifilterfalse\n","\n","\n","def lovasz_grad(gt_sorted):\n","    \"\"\"\n","    Computes gradient of the Lovasz extension w.r.t sorted errors\n","    See Alg. 1 in paper\n","    \"\"\"\n","    p = len(gt_sorted)\n","    gts = gt_sorted.sum()\n","    intersection = gts - gt_sorted.float().cumsum(0)\n","    union = gts + (1 - gt_sorted).float().cumsum(0)\n","    jaccard = 1. - intersection / union\n","    if p > 1: # cover 1-pixel case\n","        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n","    return jaccard\n","\n","\n","def iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n","    \"\"\"\n","    IoU for foreground class\n","    binary: 1 foreground, 0 background\n","    \"\"\"\n","    if not per_image:\n","        preds, labels = (preds,), (labels,)\n","    ious = []\n","    for pred, label in zip(preds, labels):\n","        intersection = ((label == 1) & (pred == 1)).sum()\n","        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n","        if not union:\n","            iou = EMPTY\n","        else:\n","            iou = float(intersection) / float(union)\n","        ious.append(iou)\n","    iou = mean(ious)    # mean accross images if per_image\n","    return 100 * iou\n","\n","\n","def iou(preds, labels, C, EMPTY=1., ignore=None, per_image=False):\n","    \"\"\"\n","    Array of IoU for each (non ignored) class\n","    \"\"\"\n","    if not per_image:\n","        preds, labels = (preds,), (labels,)\n","    ious = []\n","    for pred, label in zip(preds, labels):\n","        iou = []    \n","        for i in range(C):\n","            if i != ignore: # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n","                intersection = ((label == i) & (pred == i)).sum()\n","                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n","                if not union:\n","                    iou.append(EMPTY)\n","                else:\n","                    iou.append(float(intersection) / float(union))\n","        ious.append(iou)\n","    ious = [mean(iou) for iou in zip(*ious)] # mean accross images if per_image\n","    return 100 * np.array(ious)\n","\n","\n","# --------------------------- BINARY LOSSES ---------------------------\n","\n","\n","def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n","    \"\"\"\n","    Binary Lovasz hinge loss\n","      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n","      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n","      per_image: compute the loss per image instead of per batch\n","      ignore: void class id\n","    \"\"\"\n","    if per_image:\n","        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n","                          for log, lab in zip(logits, labels))\n","    else:\n","        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n","    return loss\n","\n","\n","def lovasz_hinge_flat(logits, labels):\n","    \"\"\"\n","    Binary Lovasz hinge loss\n","      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n","      labels: [P] Tensor, binary ground truth labels (0 or 1)\n","      ignore: label to ignore\n","    \"\"\"\n","    if len(labels) == 0:\n","        # only void pixels, the gradients should be 0\n","        return logits.sum() * 0.\n","    signs = 2. * labels.float() - 1.\n","    errors = (1. - logits * Variable(signs))\n","    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n","    perm = perm.data\n","    gt_sorted = labels[perm]\n","    grad = lovasz_grad(gt_sorted)\n","    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n","    return loss\n","\n","\n","def flatten_binary_scores(scores, labels, ignore=None):\n","    \"\"\"\n","    Flattens predictions in the batch (binary case)\n","    Remove labels equal to 'ignore'\n","    \"\"\"\n","    scores = scores.view(-1)\n","    labels = labels.view(-1)\n","    if ignore is None:\n","        return scores, labels\n","    valid = (labels != ignore)\n","    vscores = scores[valid]\n","    vlabels = labels[valid]\n","    return vscores, vlabels\n","\n","\n","class StableBCELoss(torch.nn.modules.Module):\n","    def __init__(self):\n","         super(StableBCELoss, self).__init__()\n","    def forward(self, input, target):\n","         neg_abs = - input.abs()\n","         loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n","         return loss.mean()\n","\n","\n","def binary_xloss(logits, labels, ignore=None):\n","    \"\"\"\n","    Binary Cross entropy loss\n","      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n","      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n","      ignore: void class id\n","    \"\"\"\n","    logits, labels = flatten_binary_scores(logits, labels, ignore)\n","    loss = StableBCELoss()(logits, Variable(labels.float()))\n","    return loss\n","\n","\n","# --------------------------- MULTICLASS LOSSES ---------------------------\n","\n","\n","def lovasz_softmax(probas, labels, classes='present', per_image=False, ignore=None):\n","    \"\"\"\n","    Multi-class Lovasz-Softmax loss\n","      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1).\n","              Interpreted as binary (sigmoid) output with outputs of size [B, H, W].\n","      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n","      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n","      per_image: compute the loss per image instead of per batch\n","      ignore: void class labels\n","    \"\"\"\n","    if per_image:\n","        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), classes=classes)\n","                          for prob, lab in zip(probas, labels))\n","    else:\n","        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), classes=classes)\n","    return loss\n","\n","\n","def lovasz_softmax_flat(probas, labels, classes='present'):\n","    \"\"\"\n","    Multi-class Lovasz-Softmax loss\n","      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n","      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n","      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n","    \"\"\"\n","    if probas.numel() == 0:\n","        # only void pixels, the gradients should be 0\n","        return probas * 0.\n","    C = probas.size(1)\n","    losses = []\n","    class_to_sum = list(range(C)) if classes in ['all', 'present'] else classes\n","    for c in class_to_sum:\n","        fg = (labels == c).float() # foreground for class c\n","        if (classes is 'present' and fg.sum() == 0):\n","            continue\n","        if C == 1:\n","            if len(classes) > 1:\n","                raise ValueError('Sigmoid output possible only with 1 class')\n","            class_pred = probas[:, 0]\n","        else:\n","            class_pred = probas[:, c]\n","        errors = (Variable(fg) - class_pred).abs()\n","        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n","        perm = perm.data\n","        fg_sorted = fg[perm]\n","        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n","    return mean(losses)\n","\n","\n","def flatten_probas(probas, labels, ignore=None):\n","    \"\"\"\n","    Flattens predictions in the batch\n","    \"\"\"\n","    if probas.dim() == 3:\n","        # assumes output of a sigmoid layer\n","        B, H, W = probas.size()\n","        probas = probas.view(B, 1, H, W)\n","    B, C, H, W = probas.size()\n","    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n","    labels = labels.view(-1)\n","    if ignore is None:\n","        return probas, labels\n","    valid = (labels != ignore)\n","    vprobas = probas[valid.nonzero().squeeze()]\n","    vlabels = labels[valid]\n","    return vprobas, vlabels\n","\n","def xloss(logits, labels, ignore=None):\n","    \"\"\"\n","    Cross entropy loss\n","    \"\"\"\n","    return F.cross_entropy(logits, Variable(labels), ignore_index=255)\n","\n","\n","# --------------------------- HELPER FUNCTIONS ---------------------------\n","def isnan(x):\n","    return x != x\n","    \n","    \n","def mean(l, ignore_nan=False, empty=0):\n","    \"\"\"\n","    nanmean compatible with generators.\n","    \"\"\"\n","    l = iter(l)\n","    if ignore_nan:\n","        l = ifilterfalse(isnan, l)\n","    try:\n","        n = 1\n","        acc = next(l)\n","    except StopIteration:\n","        if empty == 'raise':\n","            raise ValueError('Empty mean')\n","        return empty\n","    for n, v in enumerate(l, 2):\n","        acc += v\n","    if n == 1:\n","        return acc\n","    return acc / n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7tOt0NiQTyj"},"outputs":[],"source":["'''\n","util files\n","'''\n","def create_net(in_channels, out_channels, net_name='unet'):\n","    \"\"\"\n","    创建网络\n","    :param in_channels: 输入通道数\n","    :param out_channels: 输出通道数\n","    # :param net_name: 网络类型，可选 unet | unet_resnet18/34/50/101/152 |unet_resnext50_32x4d | deeplabv3p\n","    :param net_name: 网络类型，可选 unet | unet_resnet34\n","    \"\"\"\n","    if net_name == 'unet':\n","        net = unet(in_channels, out_channels)\n","    elif net_name == 'unet_resnet34':\n","        net = unet_resnet('resnet34', in_channels, out_channels)\n","    elif net_name == 'unet_resnet50':\n","        net = unet_resnet('resnet50', in_channels, out_channels)\n","    elif net_name == 'unet_resnet101':\n","        net = unet_resnet('resnet101', in_channels, out_channels)    \n","    elif net_name == 'resnext50_32x4d':\n","        net = unet_resnet('resnext50_32x4d', in_channels, out_channels)\n","    else:\n","        raise ValueError('Not supported net_name: {}'.format(net_name))\n","\n","    return net\n","\n","def create_loss(predicts: torch.Tensor, labels: torch.Tensor, num_classes):\n","    \"\"\"\n","    创建loss\n","    @param predicts: shape=(n, c, h, w)\n","    @param labels: shape=(n, h, w) or shape=(n, 1, h, w)\n","    @param num_classes: int should equal to channels of predicts\n","    @return: loss, mean_iou\n","    \"\"\"\n","    # permute to (n, h, w, c)\n","    predicts = predicts.permute((0, 2, 3, 1))\n","    # reshape to (-1, num_classes)  每个像素在每种分类上都有一个概率\n","    predicts = predicts.reshape((-1, num_classes))\n","    ##print(predicts.shape)\n","    ##print(labels.flatten().shape)\n","    # BCE with DICE\n","    bce_loss = F.cross_entropy(predicts, labels.flatten(), reduction='mean')  # 函数内会自动做softmax\n","    \n","    # 将labels做one_hot处理，得到的形状跟predicts相同\n","    labels_one_hot = utils.make_one_hot(labels.reshape((-1, 1)), num_classes)\n","    dice_loss = utils.DiceLoss()(predicts, labels_one_hot.to(labels.device))  # torch没有原生的，从老师给的代码里拿过来用\n","    #loss = bce_loss + dice_loss\n","    loss = bce_loss\n","    ious = compute_iou(predicts, labels.reshape((-1, 1)), num_classes)\n","    return loss, torch.mean(ious)\n","\n","def compute_iou(predicts, labels, num_classes):\n","    \"\"\"\n","    计算iou\n","    @param predicts: shape=(-1, classes)\n","    @param labels: shape=(-1, 1)\n","    \"\"\"\n","    ious = torch.zeros(num_classes)\n","    predicts = F.softmax(predicts, dim=1)\n","    predicts = torch.argmax(predicts, dim=1, keepdim=True)\n","    for i in range(num_classes):\n","        intersect = torch.sum((predicts == i) * (labels == i))\n","        area = torch.sum(predicts == i) + torch.sum(labels == i) - intersect\n","        ious[i] = intersect / (area + 1e-6)\n","    return ious\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y2sty2Y2Zx9s"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1aCHa7DvCZH_Gr9EVQrLm_819ArWQGHza"},"id":"gl-yCIdGOCQN","outputId":"210cac21-6524-431e-cd65-4c237db2d8e7"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["\"\"\"\n","@description: 执行训练\n","\"\"\"\n","\n","\n","\"\"\"\n","import\n","\"\"\"\n","#from config import ConfigTrain\n","import utils\n","from os.path import join as pjoin\n","import pandas as pd\n","import numpy as np\n","import cv2\n","import torch\n","import time\n","\n","\"\"\"\n","main\n","\"\"\"\n","from tqdm import tqdm\n","if __name__ == '__main__':\n","    cfg = ConfigTrain()\n","    print('Pick device: ', cfg.DEVICE)\n","    device = torch.device(cfg.DEVICE)\n","\n","    # 网络\n","    print('Generating net: ', cfg.NET_NAME)\n","    net = create_net(3, cfg.NUM_CLASSES, net_name=cfg.NET_NAME)\n","    if cfg.PRETRAIN:  # 加载预训练权重\n","        print('Load pretrain weights: ', cfg.PRETRAINED_WEIGHTS)\n","        net.load_state_dict(torch.load(cfg.PRETRAINED_WEIGHTS, map_location='cpu'))\n","    net.to(device)\n","    # 优化器\n","    optimizer = torch.optim.Adam(net.parameters(), lr=cfg.BASE_LR) \n","\n","    # 训练数据生成器\n","    print('Preparing data... batch_size: {}, image_size: {}, crop_offset: {}'.format(cfg.BATCH_SIZE, cfg.IMAGE_SIZE, cfg.HEIGHT_CROP_OFFSET))\n","    df_train = pd.read_csv(pjoin(cfg.DATA_LIST_ROOT, 'train.csv'))\n","    data_generator = utils.train_data_generator(np.array(df_train['image']),\n","                                                np.array(df_train['label']),\n","                                                cfg.BATCH_SIZE, cfg.IMAGE_SIZE, cfg.HEIGHT_CROP_OFFSET)\n","\n","    # 训练\n","    print('Let us train ...')\n","    log_iters = 1  # 多少次迭代打印一次log\n","    epoch_size = int(len(df_train) / cfg.BATCH_SIZE)  # 一个轮次包含的迭代次数\n","    ##trn_loss_hist = []\n","    ##iou_hist = []\n","    loss_plot = []\n","    iou_plot = []\n","    for epoch in range(cfg.EPOCH_BEGIN, cfg.EPOCH_NUM):\n","        epoch_loss = 0.0\n","        epoch_miou = 0.0\n","        last_epoch_miou = 0.0\n","        prev_time = time.time()\n","        for iteration in tqdm(range(1 , epoch_size + 1)):\n","            images, labels, images_filename = next(data_generator)\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            lr = utils.ajust_learning_rate(optimizer, cfg.LR_STRATEGY, epoch, iteration-1, epoch_size)\n","\n","            predicts = net(images)\n","\n","            optimizer.zero_grad()\n","\n","            # create loss\n","            cross_loss, mean_iou = utils.create_loss(predicts, labels, cfg.NUM_CLASSES)\n","            #iou = utils.iou(predicts, labels, 3,ignore=255, per_image=True)\n","            predicts =  torch.nn.functional.softmax(predicts,dim=1)\n","            #f_loss = focal_loss(predicts,labels)\n","            \n","            loss_lovasz_softmax = utils.lovasz_softmax(predicts, labels)\n","            loss = cross_loss + loss_lovasz_softmax\n","\n","            epoch_loss += loss.item()\n","            epoch_miou += mean_iou.item()\n","\n","            print(\"[Epoch-%d Iter-%d] LR: %.4f: iter loss: %.3f, iter iou: %.3f, epoch loss: %.3f, epoch iou: %.3f,  time cost: %.3f s\"\n","                % (epoch, iteration, lr, loss.item(), mean_iou.item(), epoch_loss / iteration, epoch_miou / iteration, time.time() - prev_time))\n","            prev_time = time.time()\n","\n","            # if mean_iou.item() < last_epoch_miou * cfg.SUSPICIOUS_RATE:\n","            #   ## TO DO: define log file or create a log file##\n","            #     with open(cfg.LOG_SUSPICIOUS_FILES, 'a+') as f:\n","            #         for filename in images_filename:\n","            #             f.write(\"{}\\n\".format(filename))\n","            #         f.flush()\n","\n","            # last_epoch_miou = epoch_miou / iteration\n","            \n","            loss.backward()\n","            loss_plot.append(loss.item())\n","            iou_plot.append(mean_iou.item())\n","            optimizer.step()\n","\n","        torch.save(net.state_dict(), \n","                    pjoin(cfg.WEIGHTS_SAVE_ROOT, '/content/drive/Shareddrives/EECS 545 Project/5DE/lane-detection-2019-howard/'+\"weights_ep_%d_%.3f_%.3f.pth\" \n","                            % (epoch, epoch_loss / epoch_size, epoch_miou / epoch_size)))\n","    \n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"qE_1dpNRcsVY","colab":{"base_uri":"https://localhost:8080/","height":387},"executionInfo":{"status":"ok","timestamp":1650251341362,"user_tz":240,"elapsed":798,"user":{"displayName":"Kili","userId":"18406356377422180073"}},"outputId":"4063f16e-4589-4147-8982-8f61bc310370"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 0, 'iteration')"]},"metadata":{},"execution_count":9},{"output_type":"display_data","data":{"text/plain":["<Figure size 720x360 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["import matplotlib.pyplot as plt\n","plt.figure(figsize=(10, 5))\n","plt.suptitle('dice loss + cross-entropy training')\n","plt.subplot(1, 2, 1)\n","plt.plot(loss_plot)\n","plt.ylabel('loss')\n","plt.xlabel('iteration')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(iou_plot)\n","plt.ylabel('Image-IoU (%)')\n","plt.xlabel('iteration')"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"4SzUTrZYZzMZ","executionInfo":{"status":"ok","timestamp":1650251457575,"user_tz":240,"elapsed":2358,"user":{"displayName":"Kili","userId":"18406356377422180073"}}},"outputs":[],"source":["torch.save(net.state_dict(), \"/content/drive/Shareddrives/EECS 545 Project/result.pt\")"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"OQmsLOcbaiXL","executionInfo":{"status":"ok","timestamp":1650251344625,"user_tz":240,"elapsed":2424,"user":{"displayName":"Kili","userId":"18406356377422180073"}}},"outputs":[],"source":["# torch.save(net.state_dict(), \n","                    # pjoin(cfg.WEIGHTS_SAVE_ROOT, \"/content/drive/Shareddrives/EECS 545 Project/5DE/lane-detection-2019-howard\"+\"weights_ep_%d_%.3f_%.3f.pth\" \n","                            # % (epoch, epoch_loss / epoch_size, epoch_miou / epoch_size)))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"lovas_exp5.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}